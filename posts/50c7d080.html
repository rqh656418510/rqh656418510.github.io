<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.0"><link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico"><link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico"><link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic%7CRoboto+Slab:300,300italic,400,400italic,700,700italic%7CRoboto+Mono:300,300italic,400,400italic,700,700italic&amp;display=swap&amp;subset=latin,latin-ext"><link rel="stylesheet" href="/lib/@fortawesome/fontawesome-free/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous"><link rel="stylesheet" href="/lib/animate.css/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous"><link rel="stylesheet" href="/lib/pace-js/themes/blue/pace-theme-minimal.css"><script src="/lib/pace-js/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script><script class="next-config" data-name="main" type="application/json">{"hostname":"www.techgrow.cn","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"flat"},"fold":{"enable":true,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script><meta name="description" content="本文主要介绍 Kafka 的使用教程。"><meta property="og:type" content="article"><meta property="og:title" content="Kafka 入门教程之七"><meta property="og:url" content="https://www.techgrow.cn/posts/50c7d080.html"><meta property="og:site_name" content="Clay 的技术空间"><meta property="og:description" content="本文主要介绍 Kafka 的使用教程。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.techgrow.cn/asset/2024/12/kafka-kraft-1.png"><meta property="og:image" content="https://www.techgrow.cn/asset/2024/12/kafka-code-1.png"><meta property="og:image" content="https://www.techgrow.cn/asset/2024/12/kafka-code-2.png"><meta property="og:image" content="https://www.techgrow.cn/asset/2024/12/kafka-code-3.png"><meta property="og:image" content="https://www.techgrow.cn/asset/2024/12/kafka-code-4.png"><meta property="og:image" content="https://www.techgrow.cn/asset/2024/12/kafka-code-5.png"><meta property="og:image" content="https://www.techgrow.cn/asset/2024/12/kafka-code-8.png"><meta property="og:image" content="https://www.techgrow.cn/asset/2024/12/kafka-code-9.png"><meta property="og:image" content="https://www.techgrow.cn/asset/2024/12/kafka-code-7.png"><meta property="og:image" content="https://www.techgrow.cn/asset/2024/12/kafka-code-10.png"><meta property="og:image" content="https://www.techgrow.cn/asset/2024/12/kafka-code-11.png"><meta property="og:image" content="https://www.techgrow.cn/asset/2024/12/kafka-code-6.png"><meta property="og:image" content="https://www.techgrow.cn/asset/2024/12/kafka-code-12.png"><meta property="og:image" content="https://www.techgrow.cn/asset/2024/12/kafka-code-13.png"><meta property="article:published_time" content="2022-09-23T14:13:45.000Z"><meta property="article:modified_time" content="2022-09-23T14:13:45.000Z"><meta property="article:author" content="Clay"><meta property="article:tag" content="分布式"><meta property="article:tag" content="消息队列"><meta property="article:tag" content="大数据"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://www.techgrow.cn/asset/2024/12/kafka-kraft-1.png"><link rel="canonical" href="https://www.techgrow.cn/posts/50c7d080.html"><script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.techgrow.cn/posts/50c7d080.html","path":"posts/50c7d080.html","title":"Kafka 入门教程之七"}</script><script class="next-config" data-name="calendar" type="application/json">""</script><title>Kafka 入门教程之七 | Clay 的技术空间</title><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-135294383-1"></script><script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-135294383-1","only_pageview":false,"measure_protocol_api_secret":null}</script><script src="/js/third-party/analytics/google-analytics.js"></script><script class="next-config" data-name="baidu_analytics" type="application/json">"84c09b30349a65573c5c642ff336969b"</script><script src="/js/third-party/analytics/baidu-analytics.js"></script><link rel="dns-prefetch" href="https://waline.techgrow.cn"><link rel="stylesheet" type="text/css" href="/css/injector/main.css"><link rel="preload" as="style" href="/css/injector/light.css"><link rel="preload" as="style" href="/css/injector/dark.css"><noscript><link rel="stylesheet" href="/css/noscript.css"></noscript><style>.admonition{margin:1.5625em 0;padding:.6rem;overflow:hidden;font-size:.64rem;page-break-inside:avoid;border-left:.3rem solid #42b983;border-radius:.3rem;box-shadow:0 .1rem .4rem rgba(0,0,0,.05),0 0 .05rem rgba(0,0,0,.1);background-color:#fafafa}p.admonition-title{position:relative;margin:-.6rem -.6rem .8em -.6rem!important;padding:.4rem .6rem .4rem 2.5rem;font-weight:700;background-color:rgba(66,185,131,.1)}.admonition-title::before{position:absolute;top:.9rem;left:1rem;width:12px;height:12px;background-color:#42b983;border-radius:50%;content:' '}.info>.admonition-title,.todo>.admonition-title{background-color:rgba(0,184,212,.1)}.attention>.admonition-title,.caution>.admonition-title,.warning>.admonition-title{background-color:rgba(255,145,0,.1)}.error>.admonition-title,.fail>.admonition-title,.failure>.admonition-title,.missing>.admonition-title{background-color:rgba(255,82,82,.1)}.admonition.info,.admonition.todo{border-color:#00b8d4}.admonition.attention,.admonition.caution,.admonition.warning{border-color:#ff9100}.admonition.error,.admonition.fail,.admonition.failure,.admonition.missing{border-color:#ff5252}.info>.admonition-title::before,.todo>.admonition-title::before{background-color:#00b8d4;border-radius:50%}.attention>.admonition-title::before,.caution>.admonition-title::before,.warning>.admonition-title::before{background-color:#ff9100;border-radius:50%}.error>.admonition-title::before,.fail>.admonition-title::before,.failure>.admonition-title::before,.missing>.admonition-title::before{background-color:#ff5252;border-radius:50%}.admonition>:last-child{margin-bottom:0!important}</style><link rel="alternate" href="/atom.xml" title="Clay 的技术空间" type="application/atom+xml"><style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head><body itemscope="" itemtype="http://schema.org/WebPage" class="use-motion"><script src="/lib/jquery/dist/jquery.min.js"></script><script data-pjax="">!function(){var t=window.location.host;if(-1==t.indexOf("127.0.0.1")&&-1==t.indexOf("localhost")){var o=document.createElement("script"),e=window.location.protocol.split(":")[0];o.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(o,n)}}()</script><link rel="stylesheet" href="/lib/aplayer/dist/APlayer.min.css"><div id="aplayer" style="display:none"></div><script src="/lib/aplayer/dist/APlayer.min.js"></script><script src="/lib/aplayer/dist/color-thief.js"></script><script src="/lib/aplayer-init.js"></script><script src="https://res.wx.qq.com/open/js/jweixin-1.4.0.js"></script><script>function getTitle(){var t=jQuery("meta[property='og:title']");return t?t.attr("content"):""}function getDesc(){var t=jQuery("meta[property='og:description']");return t?t.attr("content"):""}function randomString(t){for(var e="ABCDEFGHJKMNPQRSTWXYZabcdefhijkmnprstwxyz2345678",n=e.length,i="",r=0;r<t;++r)i+=e.charAt(Math.floor(Math.random()*n));return i}function initWx(t){wx.config({debug:!1,appId:t.appId,nonceStr:t.nonceStr,signature:t.signature,timestamp:t.timestamp,jsApiList:["checkJsApi","onMenuShareTimeline","onMenuShareAppMessage","onMenuShareQQ"]}),wx.ready(function(){wx.onMenuShareTimeline({title:t.title,link:t.link,imgUrl:t.imgUrl,success:function(){}}),wx.onMenuShareAppMessage({title:t.title,desc:t.desc,link:t.link,imgUrl:t.imgUrl,type:"link",dataUrl:"",success:function(){}}),wx.onMenuShareQQ({title:t.title,desc:t.desc,link:t.link,imgUrl:t.imgUrl,success:function(){},cancel:function(){}})}),wx.error(function(t){})}jQuery(function(){var e=getDesc(),n=getTitle(),i=randomString(16),r=(new Date).getTime(),a=window.location.href,t="https://open.techgrow.cn/api/wechat/js/signature?url="+a+"&noncestr="+i+"&timestamp="+r;jQuery.getJSON(t,function(t){initWx({desc:e,title:n,link:a,nonceStr:i,timestamp:r,signature:t.data,appId:"wx1fcf69355af43d41",imgUrl:"https://www.techgrow.cn/img/wx_share.jpg"})})})</script><div style="display:none"><img src="https://www.techgrow.cn/img/wx_share.jpg" alt=""></div><div class="headband"></div><main class="main"><div class="column"><header class="header" itemscope="" itemtype="http://schema.org/WPHeader"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏" role="button"><span class="toggle-line"></span><span class="toggle-line"></span><span class="toggle-line"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><p class="site-title">Clay 的技术空间</p><i class="logo-line"></i></a><p class="site-subtitle" itemprop="description">用进废退 | 艺不压身</p></div><div class="site-nav-right"><div class="toggle popup-trigger" aria-label="搜索" role="button"></div></div></div><nav class="site-nav"><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-search"><a href="/search" rel="section"><i class="fa fa-search fa-fw"></i>搜索</a></li><li class="menu-item menu-item-links"><a href="/links" rel="section"><i class="fas fa-link fa-fw"></i>友链</a></li><li class="menu-item menu-item-readingnotes"><a href="https://www.techgrow.cn/reading/" rel="section"><i class="fa fa-book-open-reader fa-fw"></i>读书笔记</a></li><li class="menu-item menu-item-commentmanage"><a href="https://waline.techgrow.cn/" rel="external nofollow" target="_blank"><i class="fa fa-comment fa-fw"></i>评论管理</a></li></ul></nav></header><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="sidebar-panel-container"><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%A7%E7%BA%B2"><span class="nav-text">大纲</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-text">前言</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90"><span class="nav-text">学习资源</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka-KRaft-%E6%A8%A1%E5%BC%8F"><span class="nav-text">Kafka-KRaft 模式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#KRaft-%E6%A8%A1%E5%BC%8F%E7%9A%84%E6%A6%82%E8%BF%B0"><span class="nav-text">KRaft 模式的概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KRaft-%E6%A8%A1%E5%BC%8F%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="nav-text">KRaft 模式的优势</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka-%E9%80%89%E4%B8%BE%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="nav-text">Kafka 选举的概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ZooKeeper-%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84%E9%80%89%E4%B8%BE"><span class="nav-text">ZooKeeper 模式下的选举</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KRaft-%E6%A8%A1%E5%BC%8F%E4%B8%8B%E7%9A%84%E9%80%89%E4%B8%BE"><span class="nav-text">KRaft 模式下的选举</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka-%E7%94%9F%E4%BA%A7%E8%80%85%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90"><span class="nav-text">Kafka 生产者源码分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="nav-text">生产者的工作原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E7%9A%84%E6%B5%81%E7%A8%8B"><span class="nav-text">发送消息的流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Main-%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">Main 线程的初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sender-%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">Sender 线程的初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE%E5%88%B0%E7%BC%93%E5%86%B2%E5%8C%BA"><span class="nav-text">发送数据到缓冲区</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sender-%E7%BA%BF%E7%A8%8B%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE"><span class="nav-text">Sender 线程发送数据</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E7%9A%84%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90"><span class="nav-text">生产者的源码分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Main-%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96-1"><span class="nav-text">Main 线程的初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sender-%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96-1"><span class="nav-text">Sender 线程的初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Main-%E7%BA%BF%E7%A8%8B%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE%E5%88%B0%E7%BC%93%E5%86%B2%E5%8C%BA"><span class="nav-text">Main 线程发送数据到缓冲区</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%91%E9%80%81%E6%80%BB%E4%BD%93%E6%B5%81%E7%A8%8B"><span class="nav-text">发送总体流程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E7%9A%84%E9%80%89%E6%8B%A9"><span class="nav-text">分区的选择</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE%E5%A4%A7%E5%B0%8F%E6%A0%A1%E9%AA%8C"><span class="nav-text">发送数据大小校验</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E6%B1%A0%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-text">内存池的使用</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sender-%E7%BA%BF%E7%A8%8B%E5%8F%91%E9%80%81%E6%95%B0%E6%8D%AE%E5%88%B0%E6%9C%8D%E5%8A%A1%E7%AB%AF"><span class="nav-text">Sender 线程发送数据到服务端</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka-%E6%B6%88%E8%B4%B9%E8%80%85%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90"><span class="nav-text">Kafka 消费者源码分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="nav-text">消费者的工作原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">消费者的初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E8%AE%A2%E9%98%85%E4%B8%BB%E9%A2%98"><span class="nav-text">消费者订阅主题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E6%8B%89%E5%8F%96%E5%92%8C%E5%A4%84%E7%90%86%E6%B6%88%E6%81%AF"><span class="nav-text">消费者拉取和处理消息</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E7%9A%84%E6%B6%88%E8%B4%B9%E6%B5%81%E7%A8%8B"><span class="nav-text">消费者组的消费流程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8B%89%E5%8F%96%E5%92%8C%E5%A4%84%E7%90%86%E6%B6%88%E6%81%AF%E7%9A%84%E6%B5%81%E7%A8%8B"><span class="nav-text">拉取和处理消息的流程</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E6%8F%90%E4%BA%A4-Offset"><span class="nav-text">消费者提交 Offset</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E6%B5%81%E7%A8%8B"><span class="nav-text">消费者组的初始化流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%9A%84%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90"><span class="nav-text">消费者的源码分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96-1"><span class="nav-text">消费者的初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E8%AE%A2%E9%98%85%E4%B8%BB%E9%A2%98-1"><span class="nav-text">消费者订阅主题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E6%8B%89%E5%8F%96%E5%92%8C%E5%A4%84%E7%90%86%E6%B6%88%E6%81%AF-1"><span class="nav-text">消费者拉取和处理消息</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E6%8B%89%E5%8F%96%E6%B6%88%E6%81%AF"><span class="nav-text">消费者拉取消息</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E5%A4%84%E7%90%86%E6%B6%88%E6%81%AF"><span class="nav-text">消费者处理消息</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E6%8F%90%E4%BA%A4-Offset-1"><span class="nav-text">消费者提交 Offset</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%89%8B%E5%8A%A8%E5%90%8C%E6%AD%A5%E6%8F%90%E4%BA%A4-Offset"><span class="nav-text">手动同步提交 Offset</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%89%8B%E5%8A%A8%E5%BC%82%E6%AD%A5%E6%8F%90%E4%BA%A4-Offset"><span class="nav-text">手动异步提交 Offset</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E6%B5%81%E7%A8%8B-1"><span class="nav-text">消费者组的初始化流程</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka-%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90"><span class="nav-text">Kafka 服务端源码分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="nav-text">服务端的工作原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%9A%84%E6%95%B4%E4%BD%93%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="nav-text">服务端的整体工作流程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%9A%84%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90"><span class="nav-text">服务端的源码分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%9A%84%E4%B8%BB%E5%90%AF%E5%8A%A8%E7%B1%BB"><span class="nav-text">服务端的主启动类</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope="" itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="Clay" src="/img/head.jpg"></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"> <span class="site-state-item-count">741</span> <span class="site-state-item-name">文章</span></div><div class="site-state-item site-state-tags"> <span class="site-state-item-count">53</span> <span class="site-state-item-name">标签</span></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/rqh656418510" title="GitHub → https://github.com/rqh656418510" rel="external nofollow" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:rong656418510@gmail.com" title="E-Mail → mailto:rong656418510@gmail.com" rel="external nofollow" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span><span class="links-of-author-item"><a href="/atom.xml" title="RSS → /atom.xml" rel="noopener me"><i class="fa fa-rss fa-fw"></i> RSS</a></span><span class="links-of-author-item"><a href="/sitemap.xml" title="SiteMap → /sitemap.xml" rel="noopener me"><i class="fa fa-sitemap fa-fw"></i> SiteMap</a></span></div></div></div></div></aside></div><div class="main-inner post posts-expand"><div class="post-block"><article itemscope="" itemtype="http://schema.org/Article" class="post-content" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.techgrow.cn/posts/50c7d080.html"><span hidden="" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="image" content="/img/head.jpg"><meta itemprop="name" content="Clay"></span><span hidden="" itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization"><meta itemprop="name" content="Clay 的技术空间"><meta itemprop="description" content="专注于 Java 后端、分布式、微服务、云原生、数据库、系统架构、大数据、云计算、虚拟化、人工智能学习的技术博客。"></span><span hidden="" itemprop="post" itemscope="" itemtype="http://schema.org/CreativeWork"><meta itemprop="name" content="Kafka 入门教程之七 | Clay 的技术空间"><meta itemprop="description" content="本文主要介绍 Kafka 的使用教程。"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> Kafka 入门教程之七</h1><div class="post-meta-container"><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2022-09-23 22:13:45" itemprop="dateCreated datePublished" datetime="2022-09-23T22:13:45+08:00">2022-09-23</time></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv"><span class="post-meta-item-icon"><i class="far fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i></span> <span class="post-meta-item-text">评论数：</span><a title="waline" href="/posts/50c7d080.html#waline" itemprop="discussionUrl"><span class="post-comments-count waline-comment-count" data-path="/posts/50c7d080.html" itemprop="commentCount"></span></a></span><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>5.6k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i></span> <span class="post-meta-item-text">阅读时长 ≈</span> <span>5 分钟</span></span></div></div></header><div class="post-body post-container" itemprop="articleBody" id="readmore-container"><h2 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h2><ul><li><a href="/posts/b6be8183.html">Kafka 入门教程之一</a>、<a href="/posts/60ddcede.html">Kafka 入门教程之二</a>、<a href="/posts/228158d3.html">Kafka 入门教程之三</a></li><li><a href="/posts/c61757ff.html">Kafka 入门教程之四</a>、<a href="/posts/ed9d5bd.html">Kafka 入门教程之五</a>、<a href="/posts/e73bffc6.html">Kafka 入门教程之六</a></li><li><a href="/posts/50c7d080.html">Kafka 入门教程之七</a></li></ul><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h3 id="学习资源"><a href="#学习资源" class="headerlink" title="学习资源"></a>学习资源</h3><ul><li><a target="_blank" rel="external nofollow" href="https://kafka.apache.org/documentation/">Kafka 官方文档</a></li><li><a href="/posts/ac64f898.html">Kafka 学习路线</a></li></ul><span id="more"></span><h2 id="Kafka-KRaft-模式"><a href="#Kafka-KRaft-模式" class="headerlink" title="Kafka-KRaft 模式"></a>Kafka-KRaft 模式</h2><h3 id="KRaft-模式的概述"><a href="#KRaft-模式的概述" class="headerlink" title="KRaft 模式的概述"></a>KRaft 模式的概述</h3><p><img data-src="../../../asset/2024/12/kafka-kraft-1.png"></p><ul><li>左图为 Kafka 的 ZooKeeer 模式架构，元数据存储在 Zookeeper 中，运行时会动态选举一个 Broker 节点作为 Controller（唯一），由 Controller 进行 Kafka 集群管理。</li><li>左图为 Kafka 的 KRaft 模式架构，不再依赖 Zookeeper 集群，而是使用多个 Controller 节点来代替 Zookeeper，元数据保存在 Controller 中，由 Controller 直接管理 Kafka 集群。</li></ul><table><thead><tr><th>特性</th><th> ZooKeeper 模式</th><th> KRaft 模式</th></tr></thead><tbody><tr><td> Leader 选举依赖性</td><td>需要 ZooKeeper 提供元数据存储和通知机制</td><td>不依赖 ZooKeeper，完全有 Kafka 自己自行实现</td></tr><tr><td>元数据管理</td><td>由 ZooKeeper 管理</td><td>由 Kafka 自己的 Raft 集群管理</td></tr><tr><td>一致性保障</td><td> ZooKeeper 提供一致性</td><td> Raft 协议保障一致性</td></tr><tr><td>引入版本</td><td> Kafka 早期版本（默认是 ZooKeeper 模式）</td><td>Kafka <code>2.8.0+</code>（KRaft 模式）</td></tr></tbody></table><div class="admonition note"><p class="admonition-title">版本说明</p><p>从 Kafka <code>2.8.0</code> 版本开始，Kafka 自身实现了 Raft 分布式一致性机制，这意味着 Kafka 集群可以脱离 ZooKeeper 独立运行。</p></div><h3 id="KRaft-模式的优势"><a href="#KRaft-模式的优势" class="headerlink" title="KRaft 模式的优势"></a>KRaft 模式的优势</h3><ul><li>Kafka 不再依赖外部服务，而是能够独立运行。</li><li>Controller 管理集群时，不再需要从 Zookeeper 中先读取数据，提高了集群性能。</li><li>由于不依赖 Zookeeper，因此 Kafka 集群扩展时不再受到 Zookeeper 读写能力的限制。</li><li>Controller 节点不再是通过动态选举来决定，而是由配置文件指定，这样开发者可以有针对性地加强。</li><li>Controller 节点支持通过配置来指定，而不是像以前一样对随机 Controller 节点的高负载束手无策。</li></ul><h2 id="Kafka-选举的概念"><a href="#Kafka-选举的概念" class="headerlink" title="Kafka 选举的概念"></a>Kafka 选举的概念</h2><h3 id="ZooKeeper-模式下的选举"><a href="#ZooKeeper-模式下的选举" class="headerlink" title="ZooKeeper 模式下的选举"></a>ZooKeeper 模式下的选举</h3><div class="admonition note"><p class="admonition-title">在 ZooKeeper 模式下，Controller 的责职</p><ul><li>Controller 负责管理和分发 Kafka 集群的元数据信息，例如 Topic、分区和副本的状态。</li><li>Controller 负责管理集群 Broker 的上下线，包括所有 Topic 的分区副本分配和分区副本 Leader 选举等工作。</li><li>当分区的 Leader 副本失效时，Controller 负责触发分区副本的 Leader 选举，并将新的 Leader 信息更新到 ZooKeeper 和其他 Broker。</li></ul></div><ul><li><p>Controller 的选举</p><ul><li>选举机制<ul><li> Kafka 集群的 Controller 是通过 ZooKeeper 进行选举的。</li><li>在 Broker 启动时，每个 Broker 都会尝试在 ZooKeeper 的 <code>/controller</code> 节点上创建一个临时节点。</li><li>成功创建该节点的 Broker 就成为 Controller，其它 Broker 则成为普通 Broker。</li></ul></li><li>触发条件<ul><li>如果当前的 Controller 因故障宕机（例如网络中断），ZooKeeper 会删除该 Controller 所在 Broker 的临时节点，然后触发新的 Controller 选举。</li></ul></li></ul></li><li><p>分区副本的 Leader 选举</p><ul><li>选举机制<ul><li>由 Controller 从 ZooKeeper 获取 ISR 列表，选出新的分区副本 Leader 并更新元数据。</li><li>优先从 ISR 列表中选择健康的副本作为 Leader。</li></ul></li><li>触发条件<ul><li>当分区的 Leader 副本失效（如 Broker 宕机）。</li><li>Controller 检测到 ISR（同步副本列表）发生变化。</li></ul></li><li>更新通知<ul><li>选举完成后，元数据更新至所有 Broker，客户端根据新 Leader 继续读写操作。</li></ul></li></ul></li><li><p>两种选举的区别</p><ul><li>Controller 的选举：由 ZooKeeper 决定，目的是选出集群的管理者。</li><li>分区副本的 Leader 选举：由 Controller 负责发起，目的是为每个分区在 ISR 列表中选出一个新的 Leader 副本。</li></ul></li></ul><div class="admonition warning"><p class="admonition-title">特别注意</p><ul><li>在 ZooKeeper 模式下，Kafka 集群中的 Controller 是通过 ZooKeeper 选举产生的，集群中同一时刻只能有一个活跃的 Controller（唯一）。其他 Broker 则处于非 Controller 状态，只处理分区的读写请求。</li><li>在 ZooKeeper 模式下，Kafka 集群中同一时刻只能有一个活跃的 Controller（唯一），这种单点的设计简化了管理，但也增加了 Controller 故障时的切换开销和延迟。</li><li>在 KRaft 模式下，Kafka 使用 Raft 协议来支持运行多个 Quorum Controller 节点，从而提高了可用性和一致性。</li></ul></div><h3 id="KRaft-模式下的选举"><a href="#KRaft-模式下的选举" class="headerlink" title="KRaft 模式下的选举"></a>KRaft 模式下的选举</h3><div class="admonition note"><p class="admonition-title">在 KRaft 模式下，Controller 的责职</p><ul><li>Controller 是负责管理元数据的专用节点。</li><li>Quorum Controller 是一个逻辑角色，可以分布在多个 Kafka Broker 上。</li><li>Quorum Controller 集群是通过 Raft 共识协议来保证元数据的一致性和高可用性的。</li></ul></div><ul><li><p>Controller 的选举</p><ul><li>在 KRaft 模式下，Controller 的选举是通过 Raft 共识算法完成的，而不再依赖 ZooKeeper。</li><li>在多个 Controller 节点中，会有一个被选举为 Leader Controller，负责处理元数据更新和变更请求。</li><li>其他 Controller 节点（Follower Controller）则通过 Raft 协议复制 Leader Controller 的元数据日志。</li></ul></li><li><p>分区副本的 Leader 选举</p><ul><li>Controller 使用 Raft 协议直接管理元数据并选举副本 Leader，无需依赖 ZooKeeper。</li><li>Controller 根据副本状态（如 ISR 列表）决定新的副本 Leader。</li><li>元数据变更后，通过 Raft 协议同步到所有 Quorum Controller 节点，无需依赖 ZooKeeper。</li></ul></li></ul><h2 id="Kafka-生产者源码分析"><a href="#Kafka-生产者源码分析" class="headerlink" title="Kafka 生产者源码分析"></a>Kafka 生产者源码分析</h2><div class="admonition note"><p class="admonition-title">提示</p><ul><li>在阅读和调试 Kafka 的源码之前，必须先搭建 Kafka 源码的阅读环境，详细教程请看 <a href="/posts/a2880619.html">这里</a>。</li><li>验证 Kafka 源码阅读成果的两个标志：第一个是能够自行调试源码，第二个是能够在源码上独立开发高阶功能。</li></ul></div><h3 id="生产者的工作原理"><a href="#生产者的工作原理" class="headerlink" title="生产者的工作原理"></a>生产者的工作原理</h3><h4 id="发送消息的流程"><a href="#发送消息的流程" class="headerlink" title="发送消息的流程"></a>发送消息的流程</h4><p><img data-src="../../../asset/2024/12/kafka-code-1.png"></p><h4 id="Main-线程的初始化"><a href="#Main-线程的初始化" class="headerlink" title="Main 线程的初始化"></a>Main 线程的初始化</h4><p><img data-src="../../../asset/2024/12/kafka-code-2.png"></p><h4 id="Sender-线程的初始化"><a href="#Sender-线程的初始化" class="headerlink" title="Sender 线程的初始化"></a>Sender 线程的初始化</h4><p><img data-src="../../../asset/2024/12/kafka-code-3.png"></p><h4 id="发送数据到缓冲区"><a href="#发送数据到缓冲区" class="headerlink" title="发送数据到缓冲区"></a>发送数据到缓冲区</h4><p><img data-src="../../../asset/2024/12/kafka-code-4.png"></p><h4 id="Sender-线程发送数据"><a href="#Sender-线程发送数据" class="headerlink" title="Sender 线程发送数据"></a>Sender 线程发送数据</h4><p><img data-src="../../../asset/2024/12/kafka-code-5.png"></p><h3 id="生产者的源码分析"><a href="#生产者的源码分析" class="headerlink" title="生产者的源码分析"></a>生产者的源码分析</h3><h4 id="Main-线程的初始化-1"><a href="#Main-线程的初始化-1" class="headerlink" title="Main 线程的初始化"></a>Main 线程的初始化</h4><p>这里的核心类是 <code>clients</code> 模块下的 <code>KafkaProducer</code> 类，最核心的是 <code>KafkaProducer()</code> 构造方法，如下所示：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line">KafkaProducer(ProducerConfig config,</span><br><span class="line">                Serializer&lt;K&gt; keySerializer,</span><br><span class="line">                Serializer&lt;V&gt; valueSerializer,</span><br><span class="line">                ProducerMetadata metadata,</span><br><span class="line">                KafkaClient kafkaClient,</span><br><span class="line">                ProducerInterceptors&lt;K, V&gt; interceptors,</span><br><span class="line">                Time time) {</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="keyword">this</span>.producerConfig = config;</span><br><span class="line">        <span class="keyword">this</span>.time = time;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取事务 ID</span></span><br><span class="line">        String transactionalId = config.getString(ProducerConfig.TRANSACTIONAL_ID_CONFIG);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取客户端 ID</span></span><br><span class="line">        <span class="keyword">this</span>.clientId = config.getString(ProducerConfig.CLIENT_ID_CONFIG);</span><br><span class="line"></span><br><span class="line">        LogContext logContext;</span><br><span class="line">        <span class="keyword">if</span> (transactionalId == <span class="keyword">null</span>)</span><br><span class="line">            logContext = <span class="keyword">new</span> LogContext(String.format(<span class="string">"[Producer clientId=%s] "</span>, clientId));</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            logContext = <span class="keyword">new</span> LogContext(String.format(<span class="string">"[Producer clientId=%s, transactionalId=%s] "</span>, clientId, transactionalId));</span><br><span class="line">        log = logContext.logger(KafkaProducer.class);</span><br><span class="line">        log.trace(<span class="string">"Starting the Kafka producer"</span>);</span><br><span class="line"></span><br><span class="line">        Map&lt;String, String&gt; metricTags = Collections.singletonMap(<span class="string">"client-id"</span>, clientId);</span><br><span class="line">        MetricConfig metricConfig = <span class="keyword">new</span> MetricConfig().samples(config.getInt(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG))</span><br><span class="line">                .timeWindow(config.getLong(ProducerConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG), TimeUnit.MILLISECONDS)</span><br><span class="line">                .recordLevel(Sensor.RecordingLevel.forName(config.getString(ProducerConfig.METRICS_RECORDING_LEVEL_CONFIG)))</span><br><span class="line">                .tags(metricTags);</span><br><span class="line">        List&lt;MetricsReporter&gt; reporters = config.getConfiguredInstances(ProducerConfig.METRIC_REPORTER_CLASSES_CONFIG,</span><br><span class="line">                MetricsReporter.class,</span><br><span class="line">                Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// JMX 监控相关的配置</span></span><br><span class="line">        JmxReporter jmxReporter = <span class="keyword">new</span> JmxReporter();</span><br><span class="line">        jmxReporter.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)));</span><br><span class="line">        reporters.add(jmxReporter);</span><br><span class="line">        MetricsContext metricsContext = <span class="keyword">new</span> KafkaMetricsContext(JMX_PREFIX,</span><br><span class="line">                config.originalsWithPrefix(CommonClientConfigs.METRICS_CONTEXT_PREFIX));</span><br><span class="line">        <span class="keyword">this</span>.metrics = <span class="keyword">new</span> Metrics(metricConfig, reporters, time, metricsContext);</span><br><span class="line">        <span class="comment">// 分区器的配置</span></span><br><span class="line">        <span class="keyword">this</span>.partitioner = config.getConfiguredInstance(</span><br><span class="line">                ProducerConfig.PARTITIONER_CLASS_CONFIG,</span><br><span class="line">                Partitioner.class,</span><br><span class="line">                Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId));</span><br><span class="line">        <span class="comment">// 重试时间间隔的配置，默认值是 100ms</span></span><br><span class="line">        <span class="keyword">long</span> retryBackoffMs = config.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG);</span><br><span class="line">        <span class="comment">// 序列化的配置</span></span><br><span class="line">        <span class="keyword">if</span> (keySerializer == <span class="keyword">null</span>) {</span><br><span class="line">            <span class="keyword">this</span>.keySerializer = config.getConfiguredInstance(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,</span><br><span class="line">                                                                                        Serializer.class);</span><br><span class="line">            <span class="keyword">this</span>.keySerializer.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)), <span class="keyword">true</span>);</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            config.ignore(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG);</span><br><span class="line">            <span class="keyword">this</span>.keySerializer = keySerializer;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">if</span> (valueSerializer == <span class="keyword">null</span>) {</span><br><span class="line">            <span class="keyword">this</span>.valueSerializer = config.getConfiguredInstance(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,</span><br><span class="line">                                                                                        Serializer.class);</span><br><span class="line">            <span class="keyword">this</span>.valueSerializer.configure(config.originals(Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId)), <span class="keyword">false</span>);</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            config.ignore(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG);</span><br><span class="line">            <span class="keyword">this</span>.valueSerializer = valueSerializer;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拦截器的配置</span></span><br><span class="line">        List&lt;ProducerInterceptor&lt;K, V&gt;&gt; interceptorList = (List) config.getConfiguredInstances(</span><br><span class="line">                ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,</span><br><span class="line">                ProducerInterceptor.class,</span><br><span class="line">                Collections.singletonMap(ProducerConfig.CLIENT_ID_CONFIG, clientId));</span><br><span class="line">        <span class="keyword">if</span> (interceptors != <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">this</span>.interceptors = interceptors;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">this</span>.interceptors = <span class="keyword">new</span> ProducerInterceptors&lt;&gt;(interceptorList);</span><br><span class="line">        ClusterResourceListeners clusterResourceListeners = configureClusterResourceListeners(keySerializer,</span><br><span class="line">                valueSerializer, interceptorList, reporters);</span><br><span class="line">        <span class="comment">// 生产者发送给 Kafka 的单条消息的最大大小，默认值是 1m</span></span><br><span class="line">        <span class="keyword">this</span>.maxRequestSize = config.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG);</span><br><span class="line">        <span class="comment">// 缓冲区大小，默认值是 32m</span></span><br><span class="line">        <span class="keyword">this</span>.totalMemorySize = config.getLong(ProducerConfig.BUFFER_MEMORY_CONFIG);</span><br><span class="line">        <span class="keyword">this</span>.compressionType = CompressionType.forName(config.getString(ProducerConfig.COMPRESSION_TYPE_CONFIG));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.maxBlockTimeMs = config.getLong(ProducerConfig.MAX_BLOCK_MS_CONFIG);</span><br><span class="line">        <span class="keyword">int</span> deliveryTimeoutMs = configureDeliveryTimeout(config, log);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.apiVersions = <span class="keyword">new</span> ApiVersions();</span><br><span class="line">        <span class="keyword">this</span>.transactionManager = configureTransactionState(config, logContext);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化 RecordAccumulator 缓冲区</span></span><br><span class="line">        <span class="comment">// batch.size，默认值是 16k</span></span><br><span class="line">        <span class="comment">// compression.type，默认值是 none</span></span><br><span class="line">        <span class="comment">// linger.ms，默认值是 0</span></span><br><span class="line">        <span class="comment">// retry.backoff.ms，默认值是 100ms</span></span><br><span class="line">        <span class="comment">// delivery.timeout.ms，默认值是 2 分钟</span></span><br><span class="line">        <span class="comment">// request.timeout.ms，默认值是 30s</span></span><br><span class="line">        <span class="keyword">this</span>.accumulator = <span class="keyword">new</span> RecordAccumulator(logContext,</span><br><span class="line">                config.getInt(ProducerConfig.BATCH_SIZE_CONFIG),</span><br><span class="line">                <span class="keyword">this</span>.compressionType,</span><br><span class="line">                lingerMs(config),</span><br><span class="line">                retryBackoffMs,</span><br><span class="line">                deliveryTimeoutMs,</span><br><span class="line">                metrics,</span><br><span class="line">                PRODUCER_METRIC_GROUP_NAME,</span><br><span class="line">                time,</span><br><span class="line">                apiVersions,</span><br><span class="line">                transactionManager,</span><br><span class="line">                <span class="keyword">new</span> BufferPool(<span class="keyword">this</span>.totalMemorySize, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), metrics, time, PRODUCER_METRIC_GROUP_NAME));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ZooKeeper 集群地址</span></span><br><span class="line">        List&lt;InetSocketAddress&gt; addresses = ClientUtils.parseAndValidateAddresses(</span><br><span class="line">                config.getList(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG),</span><br><span class="line">                config.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 获取元数据</span></span><br><span class="line">        <span class="keyword">if</span> (metadata != <span class="keyword">null</span>) {</span><br><span class="line">            <span class="keyword">this</span>.metadata = metadata;</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="keyword">this</span>.metadata = <span class="keyword">new</span> ProducerMetadata(retryBackoffMs,</span><br><span class="line">                    <span class="comment">// metadata.max.age.ms，生产者每隔多久需要更新一次元数据，默认值是 5 分钟</span></span><br><span class="line">                    config.getLong(ProducerConfig.METADATA_MAX_AGE_CONFIG),</span><br><span class="line">                    <span class="comment">// metadata.max.idle.ms，网络最大空闲时间设置，超过该阈值，就关闭该网络，默认值是 5 分钟</span></span><br><span class="line">                    config.getLong(ProducerConfig.METADATA_MAX_IDLE_CONFIG),</span><br><span class="line">                    logContext,</span><br><span class="line">                    clusterResourceListeners,</span><br><span class="line">                    Time.SYSTEM);</span><br><span class="line">            <span class="keyword">this</span>.metadata.bootstrap(addresses);</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">this</span>.errors = <span class="keyword">this</span>.metrics.sensor(<span class="string">"errors"</span>);</span><br><span class="line">        <span class="comment">// 初始化 Sender 线程</span></span><br><span class="line">        <span class="keyword">this</span>.sender = newSender(logContext, kafkaClient, <span class="keyword">this</span>.metadata);</span><br><span class="line">        String ioThreadName = NETWORK_THREAD_PREFIX + <span class="string">" | "</span> + clientId;</span><br><span class="line">        <span class="keyword">this</span>.ioThread = <span class="keyword">new</span> KafkaThread(ioThreadName, <span class="keyword">this</span>.sender, <span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">// 启动 Sender 线程</span></span><br><span class="line">        <span class="keyword">this</span>.ioThread.start();</span><br><span class="line">        config.logUnused();</span><br><span class="line">        AppInfoParser.registerAppInfo(JMX_PREFIX, clientId, metrics, time.milliseconds());</span><br><span class="line">        log.debug(<span class="string">"Kafka producer started"</span>);</span><br><span class="line">    } <span class="keyword">catch</span> (Throwable t) {</span><br><span class="line">        <span class="comment">// call close methods if internal objects are already constructed this is to prevent resource leak. see KAFKA-2121</span></span><br><span class="line">        close(Duration.ofMillis(<span class="number">0</span>), <span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">// now propagate the exception</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Failed to construct kafka producer"</span>, t);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h4 id="Sender-线程的初始化-1"><a href="#Sender-线程的初始化-1" class="headerlink" title="Sender 线程的初始化"></a>Sender 线程的初始化</h4><p>这里的核心类是 <code>clients</code> 模块下的 <code>KafkaProducer</code> 类，最核心的是 <code>newSender()</code> 方法，如下所示：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Sender <span class="title">newSender</span><span class="params">(LogContext logContext, KafkaClient kafkaClient, ProducerMetadata metadata)</span> </span>{</span><br><span class="line">    <span class="comment">// maxInflightRequests 没有返回 ACK 的请求的最大数量，默认值是 5</span></span><br><span class="line">    <span class="keyword">int</span> maxInflightRequests = configureInflightRequests(producerConfig);</span><br><span class="line">    <span class="comment">// request.timeout.ms，默认值是 30s</span></span><br><span class="line">    <span class="keyword">int</span> requestTimeoutMs = producerConfig.getInt(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG);</span><br><span class="line">    ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(producerConfig, time, logContext);</span><br><span class="line">    ProducerMetrics metricsRegistry = <span class="keyword">new</span> ProducerMetrics(<span class="keyword">this</span>.metrics);</span><br><span class="line">    Sensor throttleTimeSensor = Sender.throttleTimeSensor(metricsRegistry.senderMetrics);</span><br><span class="line"></span><br><span class="line">    KafkaClient client = kafkaClient != <span class="keyword">null</span> ? kafkaClient : <span class="keyword">new</span> NetworkClient(</span><br><span class="line">            <span class="keyword">new</span> Selector(producerConfig.getLong(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG),</span><br><span class="line">                    <span class="keyword">this</span>.metrics, time, <span class="string">"producer"</span>, channelBuilder, logContext),</span><br><span class="line">            metadata,</span><br><span class="line">            clientId,</span><br><span class="line">            <span class="comment">// maxInflightRequests 没有返回 ACK 的请求的最大数量，默认值是 5</span></span><br><span class="line">            <span class="comment">// reconnect.backoff.ms 重连时间间隔，默认值是 50ms</span></span><br><span class="line">            <span class="comment">// reconnect.backoff.max.ms 重连的总时间。每次重连失败时，呈指数增加重连时间，直至达到此最大值，默认值是 1000ms</span></span><br><span class="line">            <span class="comment">// send.buffer.bytes Socket 发送数据的缓冲区大小，默认值是 128k</span></span><br><span class="line">            <span class="comment">// receive.buffer.bytes Socket 接收数据的缓冲区大小，默认值是 32k</span></span><br><span class="line">            <span class="comment">// request.timeout.ms，默认值是 30s</span></span><br><span class="line">            <span class="comment">// socket.connection.setup.timeout.ms 生产者和服务器通信连接建立的时间。如果在超时之前没有建立连接，将关闭通信，默认值是 10s</span></span><br><span class="line">            <span class="comment">// socket.connection.setup.timeout.max.ms 生产者和服务器通信，每次连续连接失败时，连接建立超时将呈指数增加，直至达到此最大值，默认值是 30s</span></span><br><span class="line">            maxInflightRequests,</span><br><span class="line">            producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG),</span><br><span class="line">            producerConfig.getLong(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG),</span><br><span class="line">            producerConfig.getInt(ProducerConfig.SEND_BUFFER_CONFIG),</span><br><span class="line">            producerConfig.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG),</span><br><span class="line">            requestTimeoutMs,</span><br><span class="line">            producerConfig.getLong(ProducerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MS_CONFIG),</span><br><span class="line">            producerConfig.getLong(ProducerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MAX_MS_CONFIG),</span><br><span class="line">            time,</span><br><span class="line">            <span class="keyword">true</span>,</span><br><span class="line">            apiVersions,</span><br><span class="line">            throttleTimeSensor,</span><br><span class="line">            logContext);</span><br><span class="line">    <span class="comment">// acks，默认值是 -1</span></span><br><span class="line">    <span class="comment">// acks=0，生产者发送给 Kafka 服务器后，不需要等待应答</span></span><br><span class="line">    <span class="comment">// acks=1，生产者发送给 Kafka 服务器后，需要等待 Leader 接收后应答</span></span><br><span class="line">    <span class="comment">// acks=-1(all)，生产者发送给 Kafka 服务器后，需要等待 Leader 和 ISR 队列中的所有 Follower 接收后应答</span></span><br><span class="line">    <span class="keyword">short</span> acks = configureAcks(producerConfig, log);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Sender(logContext,</span><br><span class="line">            client,</span><br><span class="line">            metadata,</span><br><span class="line">            <span class="keyword">this</span>.accumulator,</span><br><span class="line">            maxInflightRequests == <span class="number">1</span>,</span><br><span class="line">            <span class="comment">// 生产者发送给 Kafka 的单条消息的最大大小，默认值是 1m</span></span><br><span class="line">            producerConfig.getInt(ProducerConfig.MAX_REQUEST_SIZE_CONFIG),</span><br><span class="line">            acks,</span><br><span class="line">            <span class="comment">// retries 失败重试次数，默认值是 Int 的最大值</span></span><br><span class="line">            producerConfig.getInt(ProducerConfig.RETRIES_CONFIG),</span><br><span class="line">            metricsRegistry.senderMetrics,</span><br><span class="line">            time,</span><br><span class="line">            requestTimeoutMs,</span><br><span class="line">            <span class="comment">// retry.backoff.ms 失败重试的时间间隔，默认值 100ms</span></span><br><span class="line">            producerConfig.getLong(ProducerConfig.RETRY_BACKOFF_MS_CONFIG),</span><br><span class="line">            <span class="keyword">this</span>.transactionManager,</span><br><span class="line">            apiVersions);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>Sendler 类实现了 Runnable 接口，Main 线程会将 Sender 对象放到一个线程（KafkaThread 类）中启动。Sender 类的 <code>run()</code> 方法如下所示：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{</span><br><span class="line">    log.debug(<span class="string">"Starting Kafka producer I/O thread."</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// main loop, runs until close is called</span></span><br><span class="line">    <span class="keyword">while</span> (running) {</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            <span class="comment">// Sender 线程从缓冲区拉取数据，然后将数据发送给 Kafka，刚启动时会拉取不到数据</span></span><br><span class="line">            runOnce();</span><br><span class="line">        } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">            log.error(<span class="string">"Uncaught error in kafka producer I/O thread: "</span>, e);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    log.debug(<span class="string">"Beginning shutdown of Kafka producer I/O thread, sending remaining records."</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// okay we stopped accepting requests but there may still be</span></span><br><span class="line">    <span class="comment">// requests in the transaction manager, accumulator or waiting for acknowledgment,</span></span><br><span class="line">    <span class="comment">// wait until these are completed.</span></span><br><span class="line">    <span class="keyword">while</span> (!forceClose &amp;&amp; ((<span class="keyword">this</span>.accumulator.hasUndrained() || <span class="keyword">this</span>.client.inFlightRequestCount() &gt; <span class="number">0</span>) || hasPendingTransactionalRequests())) {</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            runOnce();</span><br><span class="line">        } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">            log.error(<span class="string">"Uncaught error in kafka producer I/O thread: "</span>, e);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Abort the transaction if any commit or abort didn't go through the transaction manager's queue</span></span><br><span class="line">    <span class="keyword">while</span> (!forceClose &amp;&amp; transactionManager != <span class="keyword">null</span> &amp;&amp; transactionManager.hasOngoingTransaction()) {</span><br><span class="line">        <span class="keyword">if</span> (!transactionManager.isCompleting()) {</span><br><span class="line">            log.info(<span class="string">"Aborting incomplete transaction due to shutdown"</span>);</span><br><span class="line">            transactionManager.beginAbort();</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            runOnce();</span><br><span class="line">        } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">            log.error(<span class="string">"Uncaught error in kafka producer I/O thread: "</span>, e);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (forceClose) {</span><br><span class="line">        <span class="comment">// We need to fail all the incomplete transactional requests and batches and wake up the threads waiting on</span></span><br><span class="line">        <span class="comment">// the futures.</span></span><br><span class="line">        <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>) {</span><br><span class="line">            log.debug(<span class="string">"Aborting incomplete transactional requests due to forced shutdown"</span>);</span><br><span class="line">            transactionManager.close();</span><br><span class="line">        }</span><br><span class="line">        log.debug(<span class="string">"Aborting incomplete batches due to forced shutdown"</span>);</span><br><span class="line">        <span class="keyword">this</span>.accumulator.abortIncompleteBatches();</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="keyword">this</span>.client.close();</span><br><span class="line">    } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">        log.error(<span class="string">"Failed to close network client"</span>, e);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    log.debug(<span class="string">"Shutdown of Kafka producer I/O thread has completed."</span>);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h4 id="Main-线程发送数据到缓冲区"><a href="#Main-线程发送数据到缓冲区" class="headerlink" title="Main 线程发送数据到缓冲区"></a>Main 线程发送数据到缓冲区</h4><h5 id="发送总体流程"><a href="#发送总体流程" class="headerlink" title="发送总体流程"></a>发送总体流程</h5><p>这里的核心类是 <code>clients</code> 模块下的 <code>KafkaProducer</code> 类，最核心的是 <code>send()</code> 方法，如下所示：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Future&lt;RecordMetadata&gt; <span class="title">send</span><span class="params">(ProducerRecord&lt;K, V&gt; record, Callback callback)</span> </span>{</span><br><span class="line">    <span class="comment">// 拦截器处理待发送的数据</span></span><br><span class="line">    ProducerRecord&lt;K, V&gt; interceptedRecord = <span class="keyword">this</span>.interceptors.onSend(record);</span><br><span class="line">    <span class="comment">// 发送数据给 Kafka</span></span><br><span class="line">    <span class="keyword">return</span> doSend(interceptedRecord, callback);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><code>send()</code> 会调用 <code>onSend()</code> 方法让拦截器处理待发送的数据，如下所示：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> ProducerRecord&lt;K, V&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record)</span> </span>{</span><br><span class="line">    ProducerRecord&lt;K, V&gt; interceptRecord = record;</span><br><span class="line">    <span class="keyword">for</span> (ProducerInterceptor&lt;K, V&gt; interceptor : <span class="keyword">this</span>.interceptors) {</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            <span class="comment">// 拦截器处理</span></span><br><span class="line">            interceptRecord = interceptor.onSend(interceptRecord);</span><br><span class="line">        } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">            <span class="comment">// do not propagate interceptor exception, log and continue calling other interceptors</span></span><br><span class="line">            <span class="comment">// be careful not to throw exception from here</span></span><br><span class="line">            <span class="keyword">if</span> (record != <span class="keyword">null</span>)</span><br><span class="line">                log.warn(<span class="string">"Error executing interceptor onSend callback for topic: {}, partition: {}"</span>, record.topic(), record.partition(), e);</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                log.warn(<span class="string">"Error executing interceptor onSend callback"</span>, e);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> interceptRecord;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><code>send()</code> 会调用 <code>doSend()</code> 方法将数据发送给 Kafka，如下所示：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Future&lt;RecordMetadata&gt; <span class="title">doSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record, Callback callback)</span> </span>{</span><br><span class="line">    TopicPartition tp = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        throwIfProducerClosed();</span><br><span class="line">        <span class="comment">// first make sure the metadata for the topic is available</span></span><br><span class="line">        <span class="keyword">long</span> nowMs = time.milliseconds();</span><br><span class="line">        ClusterAndWaitTime clusterAndWaitTime;</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            <span class="comment">// 拉取元数据，maxBlockTimeMs 表示最多能等待多长时间</span></span><br><span class="line">            clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), nowMs, maxBlockTimeMs);</span><br><span class="line">        } <span class="keyword">catch</span> (KafkaException e) {</span><br><span class="line">            <span class="keyword">if</span> (metadata.isClosed())</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Producer closed while send in progress"</span>, e);</span><br><span class="line">            <span class="keyword">throw</span> e;</span><br><span class="line">        }</span><br><span class="line">        nowMs += clusterAndWaitTime.waitedOnMetadataMs;</span><br><span class="line">        <span class="comment">// 剩余等待时间 = 最多能等待的时间 - 用了多少时间</span></span><br><span class="line">        <span class="keyword">long</span> remainingWaitMs = Math.max(<span class="number">0</span>, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);</span><br><span class="line">        <span class="comment">// 更新集群的元数据</span></span><br><span class="line">        Cluster cluster = clusterAndWaitTime.cluster;</span><br><span class="line">        <span class="keyword">byte</span>[] serializedKey;</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            serializedKey = keySerializer.serialize(record.topic(), record.headers(), record.key());</span><br><span class="line">        } <span class="keyword">catch</span> (ClassCastException cce) {</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Can't convert key of class "</span> + record.key().getClass().getName() +</span><br><span class="line">                    <span class="string">" to class "</span> + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                    <span class="string">" specified in key.serializer"</span>, cce);</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 序列化操作</span></span><br><span class="line">        <span class="keyword">byte</span>[] serializedValue;</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            serializedValue = valueSerializer.serialize(record.topic(), record.headers(), record.value());</span><br><span class="line">        } <span class="keyword">catch</span> (ClassCastException cce) {</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Can't convert value of class "</span> + record.value().getClass().getName() +</span><br><span class="line">                    <span class="string">" to class "</span> + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +</span><br><span class="line">                    <span class="string">" specified in value.serializer"</span>, cce);</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 分区操作（根据集群的元数据）</span></span><br><span class="line">        <span class="keyword">int</span> partition = partition(record, serializedKey, serializedValue, cluster);</span><br><span class="line">        tp = <span class="keyword">new</span> TopicPartition(record.topic(), partition);</span><br><span class="line"></span><br><span class="line">        setReadOnly(record.headers());</span><br><span class="line">        Header[] headers = record.headers().toArray();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> serializedSize = AbstractRecords.estimateSizeInBytesUpperBound(apiVersions.maxUsableProduceMagic(),</span><br><span class="line">                compressionType, serializedKey, serializedValue, headers);</span><br><span class="line">        <span class="comment">// 校验发送消息的大小是否超过最大值，默认的最大值是 1m</span></span><br><span class="line">        ensureValidRecordSize(serializedSize);</span><br><span class="line">        <span class="keyword">long</span> timestamp = record.timestamp() == <span class="keyword">null</span> ? nowMs : record.timestamp();</span><br><span class="line">        <span class="keyword">if</span> (log.isTraceEnabled()) {</span><br><span class="line">            log.trace(<span class="string">"Attempting to append record {} with callback {} to topic {} partition {}"</span>, record, callback, record.topic(), partition);</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 消息发送的回调</span></span><br><span class="line">        <span class="comment">// producer callback will make sure to call both 'callback' and interceptor callback</span></span><br><span class="line">        Callback interceptCallback = <span class="keyword">new</span> InterceptorCallback&lt;&gt;(callback, <span class="keyword">this</span>.interceptors, tp);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span> &amp;&amp; transactionManager.isTransactional()) {</span><br><span class="line">            transactionManager.failIfNotReadyForSend();</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 缓冲区（默认大小是 32m），里面是默认 16k 一个批次</span></span><br><span class="line">        RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,</span><br><span class="line">                serializedValue, headers, interceptCallback, remainingWaitMs, <span class="keyword">true</span>, nowMs);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (result.abortForNewBatch) {</span><br><span class="line">            <span class="keyword">int</span> prevPartition = partition;</span><br><span class="line">            partitioner.onNewBatch(record.topic(), cluster, prevPartition);</span><br><span class="line">            partition = partition(record, serializedKey, serializedValue, cluster);</span><br><span class="line">            tp = <span class="keyword">new</span> TopicPartition(record.topic(), partition);</span><br><span class="line">            <span class="keyword">if</span> (log.isTraceEnabled()) {</span><br><span class="line">                log.trace(<span class="string">"Retrying append due to new batch creation for topic {} partition {}. The old partition was {}"</span>, record.topic(), partition, prevPartition);</span><br><span class="line">            }</span><br><span class="line">            <span class="comment">// producer callback will make sure to call both 'callback' and interceptor callback</span></span><br><span class="line">            interceptCallback = <span class="keyword">new</span> InterceptorCallback&lt;&gt;(callback, <span class="keyword">this</span>.interceptors, tp);</span><br><span class="line"></span><br><span class="line">            result = accumulator.append(tp, timestamp, serializedKey,</span><br><span class="line">                serializedValue, headers, interceptCallback, remainingWaitMs, <span class="keyword">false</span>, nowMs);</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span> &amp;&amp; transactionManager.isTransactional())</span><br><span class="line">            transactionManager.maybeAddPartitionToTransaction(tp);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 当批次满了或者创建了一个新的批次，则唤醒 Sender 线程发送数据给 Kafka</span></span><br><span class="line">        <span class="keyword">if</span> (result.batchIsFull || result.newBatchCreated) {</span><br><span class="line">            log.trace(<span class="string">"Waking up the sender since topic {} partition {} is either full or getting a new batch"</span>, record.topic(), partition);</span><br><span class="line">            <span class="keyword">this</span>.sender.wakeup();</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> result.future;</span><br><span class="line">        <span class="comment">// handling exceptions and record the errors;</span></span><br><span class="line">        <span class="comment">// for API exceptions return them in the future,</span></span><br><span class="line">        <span class="comment">// for other exceptions throw directly</span></span><br><span class="line">    } <span class="keyword">catch</span> (ApiException e) {</span><br><span class="line">        log.debug(<span class="string">"Exception occurred during message send:"</span>, e);</span><br><span class="line">        <span class="keyword">if</span> (callback != <span class="keyword">null</span>)</span><br><span class="line">            callback.onCompletion(<span class="keyword">null</span>, e);</span><br><span class="line">        <span class="keyword">this</span>.errors.record();</span><br><span class="line">        <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> FutureFailure(e);</span><br><span class="line">    } <span class="keyword">catch</span> (InterruptedException e) {</span><br><span class="line">        <span class="keyword">this</span>.errors.record();</span><br><span class="line">        <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> InterruptException(e);</span><br><span class="line">    } <span class="keyword">catch</span> (KafkaException e) {</span><br><span class="line">        <span class="keyword">this</span>.errors.record();</span><br><span class="line">        <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">        <span class="keyword">throw</span> e;</span><br><span class="line">    } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">        <span class="comment">// we notify interceptor about all exceptions, since onSend is called before anything else in this method</span></span><br><span class="line">        <span class="keyword">this</span>.interceptors.onSendError(record, tp, e);</span><br><span class="line">        <span class="keyword">throw</span> e;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h5 id="分区的选择"><a href="#分区的选择" class="headerlink" title="分区的选择"></a>分区的选择</h5><p>这里的核心类是 <code>clients</code> 模块下的 <code>KafkaProducer</code> 类，最核心的是 <code>partition()</code> 方法，如下所示：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(ProducerRecord&lt;K, V&gt; record, <span class="keyword">byte</span>[] serializedKey, <span class="keyword">byte</span>[] serializedValue, Cluster cluster)</span> </span>{</span><br><span class="line">    <span class="comment">// 如果指定了分区号，那么就直接使用该分区号</span></span><br><span class="line">    Integer partition = record.partition();</span><br><span class="line">    <span class="keyword">return</span> partition != <span class="keyword">null</span> ?</span><br><span class="line">            partition :</span><br><span class="line">            <span class="comment">// 分区器选择分区</span></span><br><span class="line">            partitioner.partition(</span><br><span class="line">                    record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>分区器都实现了 <code>Partitioner</code> 接口，默认的分区器实现类是 <code>DefaultPartitioner</code>，如下所示：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DefaultPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> StickyPartitionCache stickyPartitionCache = <span class="keyword">new</span> StickyPartitionCache();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>{}</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Compute the partition for the given record.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic The topic name</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key The key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> keyBytes serialized key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value The value to partition on or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> valueBytes serialized value to partition on or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> cluster The current cluster metadata</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>{</span><br><span class="line">        <span class="keyword">return</span> partition(topic, key, keyBytes, value, valueBytes, cluster, cluster.partitionsForTopic(topic).size());</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Compute the partition for the given record.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic The topic name</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> numPartitions The number of partitions of the given {<span class="doctag">@code</span> topic}</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key The key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> keyBytes serialized key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value The value to partition on or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> valueBytes serialized value to partition on or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> cluster The current cluster metadata</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster, <span class="keyword">int</span> numPartitions)</span> </span>{</span><br><span class="line">        <span class="comment">// 如果发送数据时没有指定 Key</span></span><br><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) {</span><br><span class="line">            <span class="keyword">return</span> stickyPartitionCache.partition(topic, cluster);</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 如果发送数据时有指定 Key，就按照 Key 的 Hash 值对分区数量取模</span></span><br><span class="line">        <span class="comment">// hash the keyBytes to choose a partition</span></span><br><span class="line">        <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>如果在发送数据时没有指定 Key，那么会调用 <code>StickyPartitionCache.partition()</code> 方法来获取分区号，如下所示：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StickyPartitionCache</span> </span>{</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ConcurrentMap&lt;String, Integer&gt; indexCache;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">StickyPartitionCache</span><span class="params">()</span> </span>{</span><br><span class="line">        <span class="keyword">this</span>.indexCache = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Cluster cluster)</span> </span>{</span><br><span class="line">        Integer part = indexCache.get(topic);</span><br><span class="line">        <span class="keyword">if</span> (part == <span class="keyword">null</span>) {</span><br><span class="line">            <span class="keyword">return</span> nextPartition(topic, cluster, -<span class="number">1</span>);</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> part;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">nextPartition</span><span class="params">(String topic, Cluster cluster, <span class="keyword">int</span> prevPartition)</span> </span>{</span><br><span class="line">        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">        Integer oldPart = indexCache.get(topic);</span><br><span class="line">        Integer newPart = oldPart;</span><br><span class="line">        <span class="comment">// Check that the current sticky partition for the topic is either not set or that the partition that </span></span><br><span class="line">        <span class="comment">// triggered the new batch matches the sticky partition that needs to be changed.</span></span><br><span class="line">        <span class="keyword">if</span> (oldPart == <span class="keyword">null</span> || oldPart == prevPartition) {</span><br><span class="line">            List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">            <span class="keyword">if</span> (availablePartitions.size() &lt; <span class="number">1</span>) {</span><br><span class="line">                Integer random = Utils.toPositive(ThreadLocalRandom.current().nextInt());</span><br><span class="line">                newPart = random % partitions.size();</span><br><span class="line">            } <span class="keyword">else</span> <span class="keyword">if</span> (availablePartitions.size() == <span class="number">1</span>) {</span><br><span class="line">                newPart = availablePartitions.get(<span class="number">0</span>).partition();</span><br><span class="line">            } <span class="keyword">else</span> {</span><br><span class="line">                <span class="keyword">while</span> (newPart == <span class="keyword">null</span> || newPart.equals(oldPart)) {</span><br><span class="line">                    <span class="keyword">int</span> random = Utils.toPositive(ThreadLocalRandom.current().nextInt());</span><br><span class="line">                    newPart = availablePartitions.get(random % availablePartitions.size()).partition();</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">            <span class="comment">// Only change the sticky partition if it is null or prevPartition matches the current sticky partition.</span></span><br><span class="line">            <span class="keyword">if</span> (oldPart == <span class="keyword">null</span>) {</span><br><span class="line">                indexCache.putIfAbsent(topic, newPart);</span><br><span class="line">            } <span class="keyword">else</span> {</span><br><span class="line">                indexCache.replace(topic, prevPartition, newPart);</span><br><span class="line">            }</span><br><span class="line">            <span class="keyword">return</span> indexCache.get(topic);</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> indexCache.get(topic);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><div class="admonition note"><p class="admonition-title">Kafka 的分区策略</p><ul><li>(1) 在指明 <code>partition</code> 的情况下，直接将指明的值作为 <code>partition</code> 值。例如：<code>partition=0</code>，那么数据会被写入分区 0。</li><li>(2) 在没有指明 <code>partition</code> 值，但有指定 <code>key</code> 的情况下，将 <code>key</code> 的 Hash 值与 <code>topic</code> 的 <code>partition</code> 数进行取余来得到 <code>partition</code> 值。例如：<code>key</code> 的 Hash 值是 5，<code>topic</code> 的 <code>partition</code> 数是 2，那么 <code>key</code> 对应的 <code>value</code> 会被写入 1 号分区。</li><li>(3) 在既没有指明 <code>partition</code> 值，又没有指定 <code>key</code> 的情况下，Kafka 会采用 <code>Sticky Partition</code> 黏性分区器，也就是会随机选择一个分区，并尽可能一直使用该分区，等该分区的 <code>batch</code> 已满或者已完成，Kafka 再随机一个分区进行使用（和上一次选的分区不同）。例如：第一次随机选择 0 号分区，等 0 号分区当前批次满了（默认 16K 大小）或者 <code>linger.ms</code> 设置的时间到了，Kafka 会再随机选择一个分区进行使用（如果还是 0 分区会继续随机选择一个分区）。</li></ul></div><h5 id="发送数据大小校验"><a href="#发送数据大小校验" class="headerlink" title="发送数据大小校验"></a>发送数据大小校验</h5><p>这里的核心类是 <code>clients</code> 模块下的 <code>KafkaProducer</code> 类，最核心的是 <code>ensureValidRecordSize()</code> 方法，如下所示：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">ensureValidRecordSize</span><span class="params">(<span class="keyword">int</span> size)</span> </span>{</span><br><span class="line">    <span class="comment">// 一次请求发送消息的最大大小，默认是 1m</span></span><br><span class="line">    <span class="keyword">if</span> (size &gt; maxRequestSize)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RecordTooLargeException(<span class="string">"The message is "</span> + size +</span><br><span class="line">                <span class="string">" bytes when serialized which is larger than "</span> + maxRequestSize + <span class="string">", which is the value of the "</span> +</span><br><span class="line">                ProducerConfig.MAX_REQUEST_SIZE_CONFIG + <span class="string">" configuration."</span>);</span><br><span class="line">    <span class="comment">// 缓冲区的大小，默认是 32m</span></span><br><span class="line">    <span class="keyword">if</span> (size &gt; totalMemorySize)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> RecordTooLargeException(<span class="string">"The message is "</span> + size +</span><br><span class="line">                <span class="string">" bytes when serialized which is larger than the total memory buffer you have configured with the "</span> +</span><br><span class="line">                ProducerConfig.BUFFER_MEMORY_CONFIG +</span><br><span class="line">                <span class="string">" configuration."</span>);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h5 id="内存池的使用"><a href="#内存池的使用" class="headerlink" title="内存池的使用"></a>内存池的使用</h5><p>在 <code>clients</code> 模块下的 <code>KafkaProducer</code> 类的 <code>doSend()</code> 方法中，有以下一段代码：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,</span><br><span class="line">        serializedValue, headers, interceptCallback, remainingWaitMs, <span class="keyword">true</span>, nowMs);</span><br></pre></td></tr></tbody></table></figure><p>RecordAccumulator 类的 <code>append()</code> 方法的代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> RecordAppendResult <span class="title">append</span><span class="params">(TopicPartition tp,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    <span class="keyword">long</span> timestamp,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    <span class="keyword">byte</span>[] key,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    <span class="keyword">byte</span>[] value,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    Header[] headers,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    Callback callback,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    <span class="keyword">long</span> maxTimeToBlock,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    <span class="keyword">boolean</span> abortOnNewBatch,</span></span></span><br><span class="line"><span class="params"><span class="function">                                    <span class="keyword">long</span> nowMs)</span> <span class="keyword">throws</span> InterruptedException </span>{</span><br><span class="line">    <span class="comment">// We keep track of the number of appending thread to make sure we do not miss batches in</span></span><br><span class="line">    <span class="comment">// abortIncompleteBatches().</span></span><br><span class="line">    appendsInProgress.incrementAndGet();</span><br><span class="line">    ByteBuffer buffer = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">if</span> (headers == <span class="keyword">null</span>) headers = Record.EMPTY_HEADERS;</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="comment">// 为每个分区，创建或者获取一个双端队列</span></span><br><span class="line">        <span class="comment">// check if we have an in-progress batch</span></span><br><span class="line">        Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);</span><br><span class="line">        <span class="keyword">synchronized</span> (dq) {</span><br><span class="line">            <span class="keyword">if</span> (closed)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Producer closed while send in progress"</span>);</span><br><span class="line">            <span class="comment">// 尝试往双端队列里面添加数据（第一次添加数据时，因为没有分配内存、批次对象，所以添加失败）</span></span><br><span class="line">            RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs);</span><br><span class="line">            <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> appendResult;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// we don't have an in-progress record batch try to allocate a new batch</span></span><br><span class="line">        <span class="keyword">if</span> (abortOnNewBatch) {</span><br><span class="line">            <span class="comment">// Return a result that will cause another call to append.</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> RecordAppendResult(<span class="keyword">null</span>, <span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">true</span>);</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="keyword">byte</span> maxUsableMagic = apiVersions.maxUsableProduceMagic();</span><br><span class="line">        <span class="comment">// 取批次大小（默认值是 16k）和消息大小两者中的最大值（上限默认是 1m），这样设计的主要原因是有可能一条消息的大小大于批次大小</span></span><br><span class="line">        <span class="keyword">int</span> size = Math.max(<span class="keyword">this</span>.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers));</span><br><span class="line">        log.trace(<span class="string">"Allocating a new {} byte message buffer for topic {} partition {} with remaining timeout {}ms"</span>, size, tp.topic(), tp.partition(), maxTimeToBlock);</span><br><span class="line">        <span class="comment">// 根据批次大小（默认值是 16k）和消息大小两者中的最大值（上限默认是 1m）分配内存</span></span><br><span class="line">        buffer = free.allocate(size, maxTimeToBlock);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Update the current time in case the buffer allocation blocked above.</span></span><br><span class="line">        nowMs = time.milliseconds();</span><br><span class="line">        <span class="keyword">synchronized</span> (dq) {</span><br><span class="line">            <span class="comment">// Need to check if producer is closed again after grabbing the dequeue lock.</span></span><br><span class="line">            <span class="keyword">if</span> (closed)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Producer closed while send in progress"</span>);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 尝试往双端队列里面添加数据（有内存，但是没有批次对象）</span></span><br><span class="line">            RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq, nowMs);</span><br><span class="line">            <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>) {</span><br><span class="line">                <span class="comment">// Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often...</span></span><br><span class="line">                <span class="keyword">return</span> appendResult;</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);</span><br><span class="line">            <span class="comment">// 根据内存大小封装批次（有内存、有批次对象）</span></span><br><span class="line">            ProducerBatch batch = <span class="keyword">new</span> ProducerBatch(tp, recordsBuilder, nowMs);</span><br><span class="line">            <span class="comment">// 尝试往批次里面添加数据</span></span><br><span class="line">            FutureRecordMetadata future = Objects.requireNonNull(batch.tryAppend(timestamp, key, value, headers,</span><br><span class="line">                    callback, nowMs));</span><br><span class="line">            <span class="comment">// 将新创建的批次放到双端队列的末尾</span></span><br><span class="line">            dq.addLast(batch);</span><br><span class="line">            incomplete.add(batch);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Don't deallocate this buffer in the finally block as it's being used in the record batch</span></span><br><span class="line">            buffer = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> RecordAppendResult(future, dq.size() &gt; <span class="number">1</span> || batch.isFull(), <span class="keyword">true</span>, <span class="keyword">false</span>);</span><br><span class="line">        }</span><br><span class="line">    } <span class="keyword">finally</span> {</span><br><span class="line">        <span class="keyword">if</span> (buffer != <span class="keyword">null</span>)</span><br><span class="line">            free.deallocate(buffer);</span><br><span class="line">        appendsInProgress.decrementAndGet();</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>RecordAccumulator 类的 <code>getOrCreateDeque()</code> 方法的代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt; batches;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> Deque&lt;ProducerBatch&gt; <span class="title">getOrCreateDeque</span><span class="params">(TopicPartition tp)</span> </span>{</span><br><span class="line">    <span class="comment">// 获取分区对应的双端队列</span></span><br><span class="line">    Deque&lt;ProducerBatch&gt; d = <span class="keyword">this</span>.batches.get(tp);</span><br><span class="line">    <span class="keyword">if</span> (d != <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">return</span> d;</span><br><span class="line">    d = <span class="keyword">new</span> ArrayDeque&lt;&gt;();</span><br><span class="line">    Deque&lt;ProducerBatch&gt; previous = <span class="keyword">this</span>.batches.putIfAbsent(tp, d);</span><br><span class="line">    <span class="keyword">if</span> (previous == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">return</span> d;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> previous;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h4 id="Sender-线程发送数据到服务端"><a href="#Sender-线程发送数据到服务端" class="headerlink" title="Sender 线程发送数据到服务端"></a>Sender 线程发送数据到服务端</h4><p>在 <code>clients</code> 模块下的 <code>KafkaProducer</code> 类的 <code>doSend()</code> 方法中，有以下一段代码：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (result.batchIsFull || result.newBatchCreated) {</span><br><span class="line">    log.trace(<span class="string">"Waking up the sender since topic {} partition {} is either full or getting a new batch"</span>, record.topic(), partition);</span><br><span class="line">    <span class="comment">// 唤醒 Sender 线程</span></span><br><span class="line">    <span class="keyword">this</span>.sender.wakeup();</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>Sendler 类实现了 <code>Runnable</code> 接口，查看 Sender 类的 <code>run()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>{</span><br><span class="line">    log.debug(<span class="string">"Starting Kafka producer I/O thread."</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// main loop, runs until close is called</span></span><br><span class="line">    <span class="keyword">while</span> (running) {</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            runOnce();</span><br><span class="line">        } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">            log.error(<span class="string">"Uncaught error in kafka producer I/O thread: "</span>, e);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    log.debug(<span class="string">"Beginning shutdown of Kafka producer I/O thread, sending remaining records."</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// okay we stopped accepting requests but there may still be</span></span><br><span class="line">    <span class="comment">// requests in the transaction manager, accumulator or waiting for acknowledgment,</span></span><br><span class="line">    <span class="comment">// wait until these are completed.</span></span><br><span class="line">    <span class="keyword">while</span> (!forceClose &amp;&amp; ((<span class="keyword">this</span>.accumulator.hasUndrained() || <span class="keyword">this</span>.client.inFlightRequestCount() &gt; <span class="number">0</span>) || hasPendingTransactionalRequests())) {</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            runOnce();</span><br><span class="line">        } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">            log.error(<span class="string">"Uncaught error in kafka producer I/O thread: "</span>, e);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    ......</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>Sender 类的 <code>runOnce()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">runOnce</span><span class="params">()</span> </span>{</span><br><span class="line">    <span class="comment">// 如果是事务操作</span></span><br><span class="line">    <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span>) {</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            transactionManager.maybeResolveSequences();</span><br><span class="line"></span><br><span class="line">            <span class="comment">// do not continue sending if the transaction manager is in a failed state</span></span><br><span class="line">            <span class="keyword">if</span> (transactionManager.hasFatalError()) {</span><br><span class="line">                RuntimeException lastError = transactionManager.lastError();</span><br><span class="line">                <span class="keyword">if</span> (lastError != <span class="keyword">null</span>)</span><br><span class="line">                    maybeAbortBatches(lastError);</span><br><span class="line">                client.poll(retryBackoffMs, time.milliseconds());</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Check whether we need a new producerId. If so, we will enqueue an InitProducerId</span></span><br><span class="line">            <span class="comment">// request which will be sent below</span></span><br><span class="line">            transactionManager.bumpIdempotentEpochAndResetIdIfNeeded();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (maybeSendAndPollTransactionalRequest()) {</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            }</span><br><span class="line">        } <span class="keyword">catch</span> (AuthenticationException e) {</span><br><span class="line">            <span class="comment">// This is already logged as error, but propagated here to perform any clean ups.</span></span><br><span class="line">            log.trace(<span class="string">"Authentication exception while processing transactional request"</span>, e);</span><br><span class="line">            transactionManager.authenticationFailed(e);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> currentTimeMs = time.milliseconds();</span><br><span class="line">    <span class="comment">// 将准备好的数据发送给 Kafka</span></span><br><span class="line">    <span class="keyword">long</span> pollTimeout = sendProducerData(currentTimeMs);</span><br><span class="line">    <span class="comment">// 等待发送的响应结果</span></span><br><span class="line">    client.poll(pollTimeout, currentTimeMs);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>Sender 类的 <code>sendProducerData()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">sendProducerData</span><span class="params">(<span class="keyword">long</span> now)</span> </span>{</span><br><span class="line">    <span class="comment">// 获取元数据</span></span><br><span class="line">    Cluster cluster = metadata.fetch();</span><br><span class="line">    <span class="comment">// get the list of partitions with data ready to send</span></span><br><span class="line">    <span class="comment">// 检查缓冲区（默认大小是 32m）的数据是否准备好</span></span><br><span class="line">    RecordAccumulator.ReadyCheckResult result = <span class="keyword">this</span>.accumulator.ready(cluster, now);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果 Leader 信息不知道，是不能发送数据的</span></span><br><span class="line">    <span class="comment">// if there are any partitions whose leaders are not known yet, force metadata update</span></span><br><span class="line">    <span class="keyword">if</span> (!result.unknownLeaderTopics.isEmpty()) {</span><br><span class="line">        <span class="comment">// The set of topics with unknown leader contains topics with leader election pending as well as</span></span><br><span class="line">        <span class="comment">// topics which may have expired. Add the topic again to metadata to ensure it is included</span></span><br><span class="line">        <span class="comment">// and request metadata update, since there are messages to send to the topic.</span></span><br><span class="line">        <span class="keyword">for</span> (String topic : result.unknownLeaderTopics)</span><br><span class="line">            <span class="keyword">this</span>.metadata.add(topic, now);</span><br><span class="line"></span><br><span class="line">        log.debug(<span class="string">"Requesting metadata update due to unknown leader topics from the batched records: {}"</span>,</span><br><span class="line">            result.unknownLeaderTopics);</span><br><span class="line">        <span class="keyword">this</span>.metadata.requestUpdate();</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 删除没有准备好的节点（不往这些节点发送数据）</span></span><br><span class="line">    <span class="comment">// remove any nodes we aren't ready to send to</span></span><br><span class="line">    Iterator&lt;Node&gt; iter = result.readyNodes.iterator();</span><br><span class="line">    <span class="keyword">long</span> notReadyTimeout = Long.MAX_VALUE;</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext()) {</span><br><span class="line">        Node node = iter.next();</span><br><span class="line">        <span class="keyword">if</span> (!<span class="keyword">this</span>.client.ready(node, now)) {</span><br><span class="line">            iter.remove();</span><br><span class="line">            notReadyTimeout = Math.min(notReadyTimeout, <span class="keyword">this</span>.client.pollDelayMs(node, now));</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将发往同一个 Broker 节点的数据，封装为一个请求批次</span></span><br><span class="line">    <span class="comment">// create produce requests</span></span><br><span class="line">    Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = <span class="keyword">this</span>.accumulator.drain(cluster, result.readyNodes, <span class="keyword">this</span>.maxRequestSize, now);</span><br><span class="line">    addToInflightBatches(batches);</span><br><span class="line">    <span class="keyword">if</span> (guaranteeMessageOrder) {</span><br><span class="line">        <span class="comment">// Mute all the partitions drained</span></span><br><span class="line">        <span class="keyword">for</span> (List&lt;ProducerBatch&gt; batchList : batches.values()) {</span><br><span class="line">            <span class="keyword">for</span> (ProducerBatch batch : batchList)</span><br><span class="line">                <span class="keyword">this</span>.accumulator.mutePartition(batch.topicPartition);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    accumulator.resetNextBatchExpiryTime();</span><br><span class="line">    List&lt;ProducerBatch&gt; expiredInflightBatches = getExpiredInflightBatches(now);</span><br><span class="line">    List&lt;ProducerBatch&gt; expiredBatches = <span class="keyword">this</span>.accumulator.expiredBatches(now);</span><br><span class="line">    expiredBatches.addAll(expiredInflightBatches);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Reset the producer id if an expired batch has previously been sent to the broker. Also update the metrics</span></span><br><span class="line">    <span class="comment">// for expired batches. see the documentation of @TransactionState.resetIdempotentProducerId to understand why</span></span><br><span class="line">    <span class="comment">// we need to reset the producer id here.</span></span><br><span class="line">    <span class="keyword">if</span> (!expiredBatches.isEmpty())</span><br><span class="line">        log.trace(<span class="string">"Expired {} batches in accumulator"</span>, expiredBatches.size());</span><br><span class="line">    <span class="keyword">for</span> (ProducerBatch expiredBatch : expiredBatches) {</span><br><span class="line">        String errorMessage = <span class="string">"Expiring "</span> + expiredBatch.recordCount + <span class="string">" record(s) for "</span> + expiredBatch.topicPartition</span><br><span class="line">            + <span class="string">":"</span> + (now - expiredBatch.createdMs) + <span class="string">" ms has passed since batch creation"</span>;</span><br><span class="line">        failBatch(expiredBatch, <span class="keyword">new</span> TimeoutException(errorMessage), <span class="keyword">false</span>);</span><br><span class="line">        <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span> &amp;&amp; expiredBatch.inRetry()) {</span><br><span class="line">            <span class="comment">// This ensures that no new batches are drained until the current in flight batches are fully resolved.</span></span><br><span class="line">            transactionManager.markSequenceUnresolved(expiredBatch);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    sensors.updateProduceRequestMetrics(batches);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If we have any nodes that are ready to send + have sendable data, poll with 0 timeout so this can immediately</span></span><br><span class="line">    <span class="comment">// loop and try sending more data. Otherwise, the timeout will be the smaller value between next batch expiry</span></span><br><span class="line">    <span class="comment">// time, and the delay time for checking data availability. Note that the nodes may have data that isn't yet</span></span><br><span class="line">    <span class="comment">// sendable due to lingering, backing off, etc. This specifically does not include nodes with sendable data</span></span><br><span class="line">    <span class="comment">// that aren't ready to send since they would cause busy looping.</span></span><br><span class="line">    <span class="keyword">long</span> pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);</span><br><span class="line">    pollTimeout = Math.min(pollTimeout, <span class="keyword">this</span>.accumulator.nextExpiryTimeMs() - now);</span><br><span class="line">    pollTimeout = Math.max(pollTimeout, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span> (!result.readyNodes.isEmpty()) {</span><br><span class="line">        log.trace(<span class="string">"Nodes with data ready to send: {}"</span>, result.readyNodes);</span><br><span class="line">        <span class="comment">// if some partitions are already ready to be sent, the select time would be 0;</span></span><br><span class="line">        <span class="comment">// otherwise if some partition already has some data accumulated but not ready yet,</span></span><br><span class="line">        <span class="comment">// the select time will be the time difference between now and its linger expiry time;</span></span><br><span class="line">        <span class="comment">// otherwise the select time will be the time difference between now and the metadata expiry time;</span></span><br><span class="line">        pollTimeout = <span class="number">0</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 发送请求</span></span><br><span class="line">    sendProduceRequests(batches, now);</span><br><span class="line">    <span class="keyword">return</span> pollTimeout;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>RecordAccumulator 类的 <code>ready()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 检查缓冲区（默认大小是 32m）的数据是否准备好</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ReadyCheckResult <span class="title">ready</span><span class="params">(Cluster cluster, <span class="keyword">long</span> nowMs)</span> </span>{</span><br><span class="line">    Set&lt;Node&gt; readyNodes = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">    <span class="keyword">long</span> nextReadyCheckDelayMs = Long.MAX_VALUE;</span><br><span class="line">    Set&lt;String&gt; unknownLeaderTopics = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> exhausted = <span class="keyword">this</span>.free.queued() &gt; <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt; entry : <span class="keyword">this</span>.batches.entrySet()) {</span><br><span class="line">        Deque&lt;ProducerBatch&gt; deque = entry.getValue();</span><br><span class="line">        <span class="keyword">synchronized</span> (deque) {</span><br><span class="line">            <span class="comment">// When producing to a large number of partitions, this path is hot and deques are often empty.</span></span><br><span class="line">            <span class="comment">// We check whether a batch exists first to avoid the more expensive checks whenever possible.</span></span><br><span class="line">            ProducerBatch batch = deque.peekFirst();</span><br><span class="line">            <span class="keyword">if</span> (batch != <span class="keyword">null</span>) {</span><br><span class="line">                TopicPartition part = entry.getKey();</span><br><span class="line">                Node leader = cluster.leaderFor(part);</span><br><span class="line">                <span class="keyword">if</span> (leader == <span class="keyword">null</span>) {</span><br><span class="line">                    <span class="comment">// This is a partition for which leader is not known, but messages are available to send.</span></span><br><span class="line">                    <span class="comment">// Note that entries are currently not removed from batches when deque is empty.</span></span><br><span class="line">                    unknownLeaderTopics.add(part.topic());</span><br><span class="line">                } <span class="keyword">else</span> <span class="keyword">if</span> (!readyNodes.contains(leader) &amp;&amp; !isMuted(part)) {</span><br><span class="line">                    <span class="keyword">long</span> waitedTimeMs = batch.waitedTimeMs(nowMs);</span><br><span class="line">                    <span class="comment">// 如果不是第一次拉取该批次的数据，且等待时间没有超过重试时间间隔，那么 backingOff=true</span></span><br><span class="line">                    <span class="keyword">boolean</span> backingOff = batch.attempts() &gt; <span class="number">0</span> &amp;&amp; waitedTimeMs &lt; retryBackoffMs;</span><br><span class="line">                    <span class="comment">// 如果 backingOff=true，则返回重试时间间隔（retry.backoff.ms），如果不是重试，则选择 linger.ms</span></span><br><span class="line">                    <span class="keyword">long</span> timeToWaitMs = backingOff ? retryBackoffMs : lingerMs;</span><br><span class="line">                    <span class="keyword">boolean</span> full = deque.size() &gt; <span class="number">1</span> || batch.isFull();</span><br><span class="line">                    <span class="comment">// 如果等待的时间超过了 timeToWaitMs，那么 expired=true，表示可以发送数据</span></span><br><span class="line">                    <span class="keyword">boolean</span> expired = waitedTimeMs &gt;= timeToWaitMs;</span><br><span class="line">                    <span class="keyword">boolean</span> transactionCompleting = transactionManager != <span class="keyword">null</span> &amp;&amp; transactionManager.isCompleting();</span><br><span class="line">                    <span class="keyword">boolean</span> sendable = full</span><br><span class="line">                        || expired</span><br><span class="line">                        || exhausted</span><br><span class="line">                        || closed</span><br><span class="line">                        || flushInProgress()</span><br><span class="line">                        || transactionCompleting;</span><br><span class="line">                    <span class="keyword">if</span> (sendable &amp;&amp; !backingOff) {</span><br><span class="line">                        readyNodes.add(leader);</span><br><span class="line">                    } <span class="keyword">else</span> {</span><br><span class="line">                        <span class="keyword">long</span> timeLeftMs = Math.max(timeToWaitMs - waitedTimeMs, <span class="number">0</span>);</span><br><span class="line">                        <span class="comment">// Note that this results in a conservative estimate since an un-sendable partition may have</span></span><br><span class="line">                        <span class="comment">// a leader that will later be found to have sendable data. However, this is good enough</span></span><br><span class="line">                        <span class="comment">// since we'll just wake up and then sleep again for the remaining time.</span></span><br><span class="line">                        nextReadyCheckDelayMs = Math.min(timeLeftMs, nextReadyCheckDelayMs);</span><br><span class="line">                    }</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ReadyCheckResult(readyNodes, nextReadyCheckDelayMs, unknownLeaderTopics);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>RecordAccumulator 类的 <code>drain()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将发往同一个 Broker 节点的数据，封装为一个请求批次</span></span><br><span class="line"><span class="keyword">public</span> Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; drain(Cluster cluster, Set&lt;Node&gt; nodes, <span class="keyword">int</span> maxSize, <span class="keyword">long</span> now) {</span><br><span class="line">    <span class="keyword">if</span> (nodes.isEmpty())</span><br><span class="line">        <span class="keyword">return</span> Collections.emptyMap();</span><br><span class="line"></span><br><span class="line">    Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (Node node : nodes) {</span><br><span class="line">        List&lt;ProducerBatch&gt; ready = drainBatchesForOneNode(cluster, node, maxSize, now);</span><br><span class="line">        batches.put(node.id(), ready);</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> batches;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>Sender 类的 <code>sendProduceRequest()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Transfer the record batches into a list of produce requests on a per-node basis</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendProduceRequests</span><span class="params">(Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; collated, <span class="keyword">long</span> now)</span> </span>{</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;Integer, List&lt;ProducerBatch&gt;&gt; entry : collated.entrySet())</span><br><span class="line">        sendProduceRequest(now, entry.getKey(), acks, requestTimeoutMs, entry.getValue());</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a produce request from the given record batches</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendProduceRequest</span><span class="params">(<span class="keyword">long</span> now, <span class="keyword">int</span> destination, <span class="keyword">short</span> acks, <span class="keyword">int</span> timeout, List&lt;ProducerBatch&gt; batches)</span> </span>{</span><br><span class="line">    <span class="keyword">if</span> (batches.isEmpty())</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> Map&lt;TopicPartition, ProducerBatch&gt; recordsByPartition = <span class="keyword">new</span> HashMap&lt;&gt;(batches.size());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// find the minimum magic version used when creating the record sets</span></span><br><span class="line">    <span class="keyword">byte</span> minUsedMagic = apiVersions.maxUsableProduceMagic();</span><br><span class="line">    <span class="keyword">for</span> (ProducerBatch batch : batches) {</span><br><span class="line">        <span class="keyword">if</span> (batch.magic() &lt; minUsedMagic)</span><br><span class="line">            minUsedMagic = batch.magic();</span><br><span class="line">    }</span><br><span class="line">    ProduceRequestData.TopicProduceDataCollection tpd = <span class="keyword">new</span> ProduceRequestData.TopicProduceDataCollection();</span><br><span class="line">    <span class="keyword">for</span> (ProducerBatch batch : batches) {</span><br><span class="line">        TopicPartition tp = batch.topicPartition;</span><br><span class="line">        MemoryRecords records = batch.records();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// down convert if necessary to the minimum magic used. In general, there can be a delay between the time</span></span><br><span class="line">        <span class="comment">// that the producer starts building the batch and the time that we send the request, and we may have</span></span><br><span class="line">        <span class="comment">// chosen the message format based on out-dated metadata. In the worst case, we optimistically chose to use</span></span><br><span class="line">        <span class="comment">// the new message format, but found that the broker didn't support it, so we need to down-convert on the</span></span><br><span class="line">        <span class="comment">// client before sending. This is intended to handle edge cases around cluster upgrades where brokers may</span></span><br><span class="line">        <span class="comment">// not all support the same message format version. For example, if a partition migrates from a broker</span></span><br><span class="line">        <span class="comment">// which is supporting the new magic version to one which doesn't, then we will need to convert.</span></span><br><span class="line">        <span class="keyword">if</span> (!records.hasMatchingMagic(minUsedMagic))</span><br><span class="line">            records = batch.records().downConvert(minUsedMagic, <span class="number">0</span>, time).records();</span><br><span class="line">        ProduceRequestData.TopicProduceData tpData = tpd.find(tp.topic());</span><br><span class="line">        <span class="keyword">if</span> (tpData == <span class="keyword">null</span>) {</span><br><span class="line">            tpData = <span class="keyword">new</span> ProduceRequestData.TopicProduceData().setName(tp.topic());</span><br><span class="line">            tpd.add(tpData);</span><br><span class="line">        }</span><br><span class="line">        tpData.partitionData().add(<span class="keyword">new</span> ProduceRequestData.PartitionProduceData()</span><br><span class="line">                .setIndex(tp.partition())</span><br><span class="line">                .setRecords(records));</span><br><span class="line">        recordsByPartition.put(tp, batch);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    String transactionalId = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">if</span> (transactionManager != <span class="keyword">null</span> &amp;&amp; transactionManager.isTransactional()) {</span><br><span class="line">        transactionalId = transactionManager.transactionalId();</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    ProduceRequest.Builder requestBuilder = ProduceRequest.forMagic(minUsedMagic,</span><br><span class="line">            <span class="keyword">new</span> ProduceRequestData()</span><br><span class="line">                    .setAcks(acks)</span><br><span class="line">                    .setTimeoutMs(timeout)</span><br><span class="line">                    .setTransactionalId(transactionalId)</span><br><span class="line">                    .setTopicData(tpd));</span><br><span class="line">    RequestCompletionHandler callback = response -&gt; handleProduceResponse(response, recordsByPartition, time.milliseconds());</span><br><span class="line"></span><br><span class="line">    String nodeId = Integer.toString(destination);</span><br><span class="line">    <span class="comment">// 创建发送请求对象</span></span><br><span class="line">    ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != <span class="number">0</span>,</span><br><span class="line">            requestTimeoutMs, callback);</span><br><span class="line">    <span class="comment">// 发送请求</span></span><br><span class="line">    client.send(clientRequest, now);</span><br><span class="line">    log.trace(<span class="string">"Sent produce request to {}: {}"</span>, nodeId, requestBuilder);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>NetworkClient 的 <code>send()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Queue up the given request for sending. Requests can only be sent out to ready nodes.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> request The request</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> now The current timestamp</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">(ClientRequest request, <span class="keyword">long</span> now)</span> </span>{</span><br><span class="line">    doSend(request, <span class="keyword">false</span>, now);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doSend</span><span class="params">(ClientRequest clientRequest, <span class="keyword">boolean</span> isInternalRequest, <span class="keyword">long</span> now)</span> </span>{</span><br><span class="line">    ensureActive();</span><br><span class="line">    String nodeId = clientRequest.destination();</span><br><span class="line">    <span class="keyword">if</span> (!isInternalRequest) {</span><br><span class="line">        <span class="comment">// If this request came from outside the NetworkClient, validate</span></span><br><span class="line">        <span class="comment">// that we can send data.  If the request is internal, we trust</span></span><br><span class="line">        <span class="comment">// that internal code has done this validation.  Validation</span></span><br><span class="line">        <span class="comment">// will be slightly different for some internal requests (for</span></span><br><span class="line">        <span class="comment">// example, ApiVersionsRequests can be sent prior to being in</span></span><br><span class="line">        <span class="comment">// READY state.)</span></span><br><span class="line">        <span class="keyword">if</span> (!canSendRequest(nodeId, now))</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Attempt to send a request to node "</span> + nodeId + <span class="string">" which is not ready."</span>);</span><br><span class="line">    }</span><br><span class="line">    AbstractRequest.Builder&lt;?&gt; builder = clientRequest.requestBuilder();</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        NodeApiVersions versionInfo = apiVersions.get(nodeId);</span><br><span class="line">        <span class="keyword">short</span> version;</span><br><span class="line">        <span class="comment">// Note: if versionInfo is null, we have no server version information. This would be</span></span><br><span class="line">        <span class="comment">// the case when sending the initial ApiVersionRequest which fetches the version</span></span><br><span class="line">        <span class="comment">// information itself.  It is also the case when discoverBrokerVersions is set to false.</span></span><br><span class="line">        <span class="keyword">if</span> (versionInfo == <span class="keyword">null</span>) {</span><br><span class="line">            version = builder.latestAllowedVersion();</span><br><span class="line">            <span class="keyword">if</span> (discoverBrokerVersions &amp;&amp; log.isTraceEnabled())</span><br><span class="line">                log.trace(<span class="string">"No version information found when sending {} with correlation id {} to node {}. "</span> +</span><br><span class="line">                        <span class="string">"Assuming version {}."</span>, clientRequest.apiKey(), clientRequest.correlationId(), nodeId, version);</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            version = versionInfo.latestUsableVersion(clientRequest.apiKey(), builder.oldestAllowedVersion(),</span><br><span class="line">                    builder.latestAllowedVersion());</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// The call to build may also throw UnsupportedVersionException, if there are essential</span></span><br><span class="line">        <span class="comment">// fields that cannot be represented in the chosen version.</span></span><br><span class="line">        <span class="comment">// 发送请求</span></span><br><span class="line">        doSend(clientRequest, isInternalRequest, now, builder.build(version));</span><br><span class="line">    } <span class="keyword">catch</span> (UnsupportedVersionException unsupportedVersionException) {</span><br><span class="line">        <span class="comment">// If the version is not supported, skip sending the request over the wire.</span></span><br><span class="line">        <span class="comment">// Instead, simply add it to the local queue of aborted requests.</span></span><br><span class="line">        log.debug(<span class="string">"Version mismatch when attempting to send {} with correlation id {} to {}"</span>, builder,</span><br><span class="line">                clientRequest.correlationId(), clientRequest.destination(), unsupportedVersionException);</span><br><span class="line">        ClientResponse clientResponse = <span class="keyword">new</span> ClientResponse(clientRequest.makeHeader(builder.latestAllowedVersion()),</span><br><span class="line">                clientRequest.callback(), clientRequest.destination(), now, now,</span><br><span class="line">                <span class="keyword">false</span>, unsupportedVersionException, <span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!isInternalRequest)</span><br><span class="line">            abortedSends.add(clientResponse);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (clientRequest.apiKey() == ApiKeys.METADATA)</span><br><span class="line">            metadataUpdater.handleFailedRequest(now, Optional.of(unsupportedVersionException));</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doSend</span><span class="params">(ClientRequest clientRequest, <span class="keyword">boolean</span> isInternalRequest, <span class="keyword">long</span> now, AbstractRequest request)</span> </span>{</span><br><span class="line">    String destination = clientRequest.destination();</span><br><span class="line">    RequestHeader header = clientRequest.makeHeader(request.version());</span><br><span class="line">    <span class="keyword">if</span> (log.isDebugEnabled()) {</span><br><span class="line">        log.debug(<span class="string">"Sending {} request with header {} and timeout {} to node {}: {}"</span>,</span><br><span class="line">            clientRequest.apiKey(), header, clientRequest.requestTimeoutMs(), destination, request);</span><br><span class="line">    }</span><br><span class="line">    Send send = request.toSend(header);</span><br><span class="line">    InFlightRequest inFlightRequest = <span class="keyword">new</span> InFlightRequest(</span><br><span class="line">            clientRequest,</span><br><span class="line">            header,</span><br><span class="line">            isInternalRequest,</span><br><span class="line">            request,</span><br><span class="line">            send,</span><br><span class="line">            now);</span><br><span class="line">    <span class="comment">// 添加发送请求到 inFlight</span></span><br><span class="line">    <span class="keyword">this</span>.inFlightRequests.add(inFlightRequest);</span><br><span class="line">    <span class="comment">// 发送数据</span></span><br><span class="line">    selector.send(<span class="keyword">new</span> NetworkSend(clientRequest.destination(), send));</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>NetworkClient 的 <code>poll()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取发送的响应结果</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;ClientResponse&gt; <span class="title">poll</span><span class="params">(<span class="keyword">long</span> timeout, <span class="keyword">long</span> now)</span> </span>{</span><br><span class="line">    ensureActive();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!abortedSends.isEmpty()) {</span><br><span class="line">        <span class="comment">// If there are aborted sends because of unsupported version exceptions or disconnects,</span></span><br><span class="line">        <span class="comment">// handle them immediately without waiting for Selector#poll.</span></span><br><span class="line">        List&lt;ClientResponse&gt; responses = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        handleAbortedSends(responses);</span><br><span class="line">        completeResponses(responses);</span><br><span class="line">        <span class="keyword">return</span> responses;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> metadataTimeout = metadataUpdater.maybeUpdate(now);</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="keyword">this</span>.selector.poll(Utils.min(timeout, metadataTimeout, defaultRequestTimeoutMs));</span><br><span class="line">    } <span class="keyword">catch</span> (IOException e) {</span><br><span class="line">        log.error(<span class="string">"Unexpected error during I/O"</span>, e);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// process completed actions</span></span><br><span class="line">    <span class="comment">// 获取发送后的响应结果</span></span><br><span class="line">    <span class="keyword">long</span> updatedNow = <span class="keyword">this</span>.time.milliseconds();</span><br><span class="line">    List&lt;ClientResponse&gt; responses = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    handleCompletedSends(responses, updatedNow);</span><br><span class="line">    handleCompletedReceives(responses, updatedNow);</span><br><span class="line">    handleDisconnections(responses, updatedNow);</span><br><span class="line">    handleConnections();</span><br><span class="line">    handleInitiateApiVersionRequests(updatedNow);</span><br><span class="line">    handleTimedOutConnections(responses, updatedNow);</span><br><span class="line">    handleTimedOutRequests(responses, updatedNow);</span><br><span class="line">    completeResponses(responses);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> responses;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h2 id="Kafka-消费者源码分析"><a href="#Kafka-消费者源码分析" class="headerlink" title="Kafka 消费者源码分析"></a>Kafka 消费者源码分析</h2><div class="admonition note"><p class="admonition-title">提示</p><ul><li>在阅读和调试 Kafka 的源码之前，必须先搭建 Kafka 源码的阅读环境，详细教程请看 <a href="/posts/a2880619.html">这里</a>。</li><li>验证 Kafka 源码阅读成果的两个标志：第一个是能够自行调试源码，第二个是能够在源码上独立开发高阶功能。</li></ul></div><h3 id="消费者的工作原理"><a href="#消费者的工作原理" class="headerlink" title="消费者的工作原理"></a>消费者的工作原理</h3><h4 id="消费者的初始化"><a href="#消费者的初始化" class="headerlink" title="消费者的初始化"></a>消费者的初始化</h4><p><img data-src="../../../asset/2024/12/kafka-code-8.png"></p><h4 id="消费者订阅主题"><a href="#消费者订阅主题" class="headerlink" title="消费者订阅主题"></a>消费者订阅主题</h4><p><img data-src="../../../asset/2024/12/kafka-code-9.png"></p><h4 id="消费者拉取和处理消息"><a href="#消费者拉取和处理消息" class="headerlink" title="消费者拉取和处理消息"></a>消费者拉取和处理消息</h4><h5 id="消费者组的消费流程"><a href="#消费者组的消费流程" class="headerlink" title="消费者组的消费流程"></a>消费者组的消费流程</h5><p><img data-src="../../../asset/2024/12/kafka-code-7.png"></p><h5 id="拉取和处理消息的流程"><a href="#拉取和处理消息的流程" class="headerlink" title="拉取和处理消息的流程"></a>拉取和处理消息的流程</h5><p><img data-src="../../../asset/2024/12/kafka-code-10.png"></p><h4 id="消费者提交-Offset"><a href="#消费者提交-Offset" class="headerlink" title="消费者提交 Offset"></a>消费者提交 Offset</h4><p><img data-src="../../../asset/2024/12/kafka-code-11.png"></p><h4 id="消费者组的初始化流程"><a href="#消费者组的初始化流程" class="headerlink" title="消费者组的初始化流程"></a>消费者组的初始化流程</h4><p><img data-src="../../../asset/2024/12/kafka-code-6.png"></p><h3 id="消费者的源码分析"><a href="#消费者的源码分析" class="headerlink" title="消费者的源码分析"></a>消费者的源码分析</h3><h4 id="消费者的初始化-1"><a href="#消费者的初始化-1" class="headerlink" title="消费者的初始化"></a>消费者的初始化</h4><p>这里的核心类是 <code>clients</code> 模块下的 KafkaConsumer 类，最核心的是 <code>KafkaConsumer()</code> 构造方法，如下所示：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line">KafkaConsumer(ConsumerConfig config, Deserializer&lt;K&gt; keyDeserializer, Deserializer&lt;V&gt; valueDeserializer) {</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="comment">// 消费者组分区再平衡的配置</span></span><br><span class="line">        GroupRebalanceConfig groupRebalanceConfig = <span class="keyword">new</span> GroupRebalanceConfig(config,</span><br><span class="line">                GroupRebalanceConfig.ProtocolType.CONSUMER);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取消费者组的 ID</span></span><br><span class="line">        <span class="keyword">this</span>.groupId = Optional.ofNullable(groupRebalanceConfig.groupId);</span><br><span class="line">        <span class="comment">// 获取客户端 ID</span></span><br><span class="line">        <span class="keyword">this</span>.clientId = config.getString(CommonClientConfigs.CLIENT_ID_CONFIG);</span><br><span class="line"></span><br><span class="line">        LogContext logContext;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// If group.instance.id is set, we will append it to the log context.</span></span><br><span class="line">        <span class="keyword">if</span> (groupRebalanceConfig.groupInstanceId.isPresent()) {</span><br><span class="line">            logContext = <span class="keyword">new</span> LogContext(<span class="string">"[Consumer instanceId="</span> + groupRebalanceConfig.groupInstanceId.get() +</span><br><span class="line">                    <span class="string">", clientId="</span> + clientId + <span class="string">", groupId="</span> + groupId.orElse(<span class="string">"null"</span>) + <span class="string">"] "</span>);</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            logContext = <span class="keyword">new</span> LogContext(<span class="string">"[Consumer clientId="</span> + clientId + <span class="string">", groupId="</span> + groupId.orElse(<span class="string">"null"</span>) + <span class="string">"] "</span>);</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.log = logContext.logger(getClass());</span><br><span class="line">        <span class="keyword">boolean</span> enableAutoCommit = config.maybeOverrideEnableAutoCommit();</span><br><span class="line">        groupId.ifPresent(groupIdStr -&gt; {</span><br><span class="line">            <span class="keyword">if</span> (groupIdStr.isEmpty()) {</span><br><span class="line">                log.warn(<span class="string">"Support for using the empty group id by consumers is deprecated and will be removed in the next major release."</span>);</span><br><span class="line">            }</span><br><span class="line">        });</span><br><span class="line"></span><br><span class="line">        log.debug(<span class="string">"Initializing the Kafka consumer"</span>);</span><br><span class="line">        <span class="comment">// 等待服务端返回响应的最大等待时间，默认是 30s</span></span><br><span class="line">        <span class="keyword">this</span>.requestTimeoutMs = config.getInt(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG);</span><br><span class="line">        <span class="keyword">this</span>.defaultApiTimeoutMs = config.getInt(ConsumerConfig.DEFAULT_API_TIMEOUT_MS_CONFIG);</span><br><span class="line">        <span class="keyword">this</span>.time = Time.SYSTEM;</span><br><span class="line">        <span class="keyword">this</span>.metrics = buildMetrics(config, time, clientId);</span><br><span class="line">        <span class="comment">// 重试时间间隔</span></span><br><span class="line">        <span class="keyword">this</span>.retryBackoffMs = config.getLong(ConsumerConfig.RETRY_BACKOFF_MS_CONFIG);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 拦截器配置</span></span><br><span class="line">        List&lt;ConsumerInterceptor&lt;K, V&gt;&gt; interceptorList = (List) config.getConfiguredInstances(</span><br><span class="line">                ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG,</span><br><span class="line">                ConsumerInterceptor.class,</span><br><span class="line">                Collections.singletonMap(ConsumerConfig.CLIENT_ID_CONFIG, clientId));</span><br><span class="line">        <span class="keyword">this</span>.interceptors = <span class="keyword">new</span> ConsumerInterceptors&lt;&gt;(interceptorList);</span><br><span class="line">        <span class="comment">// Key 的反序列化配置</span></span><br><span class="line">        <span class="keyword">if</span> (keyDeserializer == <span class="keyword">null</span>) {</span><br><span class="line">            <span class="keyword">this</span>.keyDeserializer = config.getConfiguredInstance(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, Deserializer.class);</span><br><span class="line">            <span class="keyword">this</span>.keyDeserializer.configure(config.originals(Collections.singletonMap(ConsumerConfig.CLIENT_ID_CONFIG, clientId)), <span class="keyword">true</span>);</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            config.ignore(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">            <span class="keyword">this</span>.keyDeserializer = keyDeserializer;</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// Value 的反序列化配置</span></span><br><span class="line">        <span class="keyword">if</span> (valueDeserializer == <span class="keyword">null</span>) {</span><br><span class="line">            <span class="keyword">this</span>.valueDeserializer = config.getConfiguredInstance(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, Deserializer.class);</span><br><span class="line">            <span class="keyword">this</span>.valueDeserializer.configure(config.originals(Collections.singletonMap(ConsumerConfig.CLIENT_ID_CONFIG, clientId)), <span class="keyword">false</span>);</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            config.ignore(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG);</span><br><span class="line">            <span class="keyword">this</span>.valueDeserializer = valueDeserializer;</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 如果找不到上次消费的偏移量（Offset），从哪个偏移量（Offset）开始消费，可选值：earliest | latest | none，默认值是 latest</span></span><br><span class="line">        OffsetResetStrategy offsetResetStrategy = OffsetResetStrategy.valueOf(config.getString(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG).toUpperCase(Locale.ROOT));</span><br><span class="line">        <span class="keyword">this</span>.subscriptions = <span class="keyword">new</span> SubscriptionState(logContext, offsetResetStrategy);</span><br><span class="line">        ClusterResourceListeners clusterResourceListeners = configureClusterResourceListeners(keyDeserializer,</span><br><span class="line">                valueDeserializer, metrics.reporters(), interceptorList);</span><br><span class="line">        <span class="comment">// 获取元数据</span></span><br><span class="line">        <span class="keyword">this</span>.metadata = <span class="keyword">new</span> ConsumerMetadata(retryBackoffMs,</span><br><span class="line">                config.getLong(ConsumerConfig.METADATA_MAX_AGE_CONFIG),</span><br><span class="line">                <span class="comment">// 是否可以消费系统内置主题</span></span><br><span class="line">                !config.getBoolean(ConsumerConfig.EXCLUDE_INTERNAL_TOPICS_CONFIG),</span><br><span class="line">                <span class="comment">// 是否允许自动创建主题</span></span><br><span class="line">                config.getBoolean(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG),</span><br><span class="line">                subscriptions, logContext, clusterResourceListeners);</span><br><span class="line">        <span class="comment">// Kafka 服务端的连接地址</span></span><br><span class="line">        List&lt;InetSocketAddress&gt; addresses = ClientUtils.parseAndValidateAddresses(</span><br><span class="line">                config.getList(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG), config.getString(ConsumerConfig.CLIENT_DNS_LOOKUP_CONFIG));</span><br><span class="line">        <span class="keyword">this</span>.metadata.bootstrap(addresses);</span><br><span class="line">        String metricGrpPrefix = <span class="string">"consumer"</span>;</span><br><span class="line"></span><br><span class="line">        FetcherMetricsRegistry metricsRegistry = <span class="keyword">new</span> FetcherMetricsRegistry(Collections.singleton(CLIENT_ID_METRIC_TAG), metricGrpPrefix);</span><br><span class="line">        ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(config, time, logContext);</span><br><span class="line">        <span class="keyword">this</span>.isolationLevel = IsolationLevel.valueOf(</span><br><span class="line">                config.getString(ConsumerConfig.ISOLATION_LEVEL_CONFIG).toUpperCase(Locale.ROOT));</span><br><span class="line">        Sensor throttleTimeSensor = Fetcher.throttleTimeSensor(metrics, metricsRegistry);</span><br><span class="line">        <span class="comment">// 获取心跳时间，默认值是 3s</span></span><br><span class="line">        <span class="keyword">int</span> heartbeatIntervalMs = config.getInt(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG);</span><br><span class="line"></span><br><span class="line">        ApiVersions apiVersions = <span class="keyword">new</span> ApiVersions();</span><br><span class="line">        <span class="comment">// 创建网络客户端</span></span><br><span class="line">        NetworkClient netClient = <span class="keyword">new</span> NetworkClient(</span><br><span class="line">                <span class="keyword">new</span> Selector(config.getLong(ConsumerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG), metrics, time, metricGrpPrefix, channelBuilder, logContext),</span><br><span class="line">                <span class="keyword">this</span>.metadata,</span><br><span class="line">                clientId,</span><br><span class="line">                <span class="number">100</span>, <span class="comment">// a fixed large enough value will suffice for max in-flight requests</span></span><br><span class="line">                config.getLong(ConsumerConfig.RECONNECT_BACKOFF_MS_CONFIG),</span><br><span class="line">                config.getLong(ConsumerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG),</span><br><span class="line">                config.getInt(ConsumerConfig.SEND_BUFFER_CONFIG),</span><br><span class="line">                config.getInt(ConsumerConfig.RECEIVE_BUFFER_CONFIG),</span><br><span class="line">                config.getInt(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG),</span><br><span class="line">                config.getLong(ConsumerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MS_CONFIG),</span><br><span class="line">                config.getLong(ConsumerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MAX_MS_CONFIG),</span><br><span class="line">                time,</span><br><span class="line">                <span class="keyword">true</span>,</span><br><span class="line">                apiVersions,</span><br><span class="line">                throttleTimeSensor,</span><br><span class="line">                logContext);</span><br><span class="line">        <span class="comment">// 创建消费者网络客户端</span></span><br><span class="line">        <span class="keyword">this</span>.client = <span class="keyword">new</span> ConsumerNetworkClient(</span><br><span class="line">                logContext,</span><br><span class="line">                netClient,</span><br><span class="line">                metadata,</span><br><span class="line">                time,</span><br><span class="line">                retryBackoffMs,</span><br><span class="line">                config.getInt(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG),</span><br><span class="line">                heartbeatIntervalMs); <span class="comment">//Will avoid blocking an extended period of time to prevent heartbeat thread starvation</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 获取消费者分区分配策略</span></span><br><span class="line">        <span class="keyword">this</span>.assignors = ConsumerPartitionAssignor.getAssignorInstances(</span><br><span class="line">                config.getList(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG),</span><br><span class="line">                config.originals(Collections.singletonMap(ConsumerConfig.CLIENT_ID_CONFIG, clientId))</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        <span class="comment">// no coordinator will be constructed for the default (null) group id</span></span><br><span class="line">        <span class="comment">// 创建消费者协调器</span></span><br><span class="line">        <span class="keyword">this</span>.coordinator = !groupId.isPresent() ? <span class="keyword">null</span> :</span><br><span class="line">            <span class="keyword">new</span> ConsumerCoordinator(groupRebalanceConfig,</span><br><span class="line">                    logContext,</span><br><span class="line">                    <span class="keyword">this</span>.client,</span><br><span class="line">                    assignors,</span><br><span class="line">                    <span class="keyword">this</span>.metadata,</span><br><span class="line">                    <span class="keyword">this</span>.subscriptions,</span><br><span class="line">                    metrics,</span><br><span class="line">                    metricGrpPrefix,</span><br><span class="line">                    <span class="keyword">this</span>.time,</span><br><span class="line">                    enableAutoCommit,</span><br><span class="line">                    <span class="comment">// 自动提交 Offset 的时间间隔，默认值是 5s</span></span><br><span class="line">                    config.getInt(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG),</span><br><span class="line">                    <span class="keyword">this</span>.interceptors,</span><br><span class="line">                    config.getBoolean(ConsumerConfig.THROW_ON_FETCH_STABLE_OFFSET_UNSUPPORTED));</span><br><span class="line">        <span class="comment">// 客户端从服务端抓取数据的配置</span></span><br><span class="line">        <span class="keyword">this</span>.fetcher = <span class="keyword">new</span> Fetcher&lt;&gt;(</span><br><span class="line">                logContext,</span><br><span class="line">                <span class="keyword">this</span>.client,</span><br><span class="line">                <span class="comment">// 一次抓取数据的最小字节数，默认值是 1 个字节</span></span><br><span class="line">                config.getInt(ConsumerConfig.FETCH_MIN_BYTES_CONFIG),</span><br><span class="line">                <span class="comment">// 一次抓取数据的最大字节数，默认值是 50m</span></span><br><span class="line">                config.getInt(ConsumerConfig.FETCH_MAX_BYTES_CONFIG),</span><br><span class="line">                <span class="comment">// 一次抓取数据的最大等待时间，默认 500ms</span></span><br><span class="line">                config.getInt(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG),</span><br><span class="line">                <span class="comment">// 每个分区抓取数据的最大字节数，默认 1m</span></span><br><span class="line">                config.getInt(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG),</span><br><span class="line">                <span class="comment">// 一次 poll 拉取数据的最大条数，默认是 500</span></span><br><span class="line">                config.getInt(ConsumerConfig.MAX_POLL_RECORDS_CONFIG),</span><br><span class="line">                config.getBoolean(ConsumerConfig.CHECK_CRCS_CONFIG),</span><br><span class="line">                config.getString(ConsumerConfig.CLIENT_RACK_CONFIG),</span><br><span class="line">                <span class="comment">// Key 的反序列化器</span></span><br><span class="line">                <span class="keyword">this</span>.keyDeserializer,</span><br><span class="line">                <span class="comment">// Value 的反序列化器</span></span><br><span class="line">                <span class="keyword">this</span>.valueDeserializer,</span><br><span class="line">                <span class="keyword">this</span>.metadata,</span><br><span class="line">                <span class="keyword">this</span>.subscriptions,</span><br><span class="line">                metrics,</span><br><span class="line">                metricsRegistry,</span><br><span class="line">                <span class="keyword">this</span>.time,</span><br><span class="line">                <span class="keyword">this</span>.retryBackoffMs,</span><br><span class="line">                <span class="keyword">this</span>.requestTimeoutMs,</span><br><span class="line">                isolationLevel,</span><br><span class="line">                apiVersions);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.kafkaConsumerMetrics = <span class="keyword">new</span> KafkaConsumerMetrics(metrics, metricGrpPrefix);</span><br><span class="line"></span><br><span class="line">        config.logUnused();</span><br><span class="line">        AppInfoParser.registerAppInfo(JMX_PREFIX, clientId, metrics, time.milliseconds());</span><br><span class="line">        log.debug(<span class="string">"Kafka consumer initialized"</span>);</span><br><span class="line">    } <span class="keyword">catch</span> (Throwable t) {</span><br><span class="line">        <span class="comment">// call close methods if internal objects are already constructed; this is to prevent resource leak. see KAFKA-2121</span></span><br><span class="line">        <span class="comment">// we do not need to call `close` at all when `log` is null, which means no internal objects were initialized.</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.log != <span class="keyword">null</span>) {</span><br><span class="line">            close(<span class="number">0</span>, <span class="keyword">true</span>);</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// now propagate the exception</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Failed to construct kafka consumer"</span>, t);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h4 id="消费者订阅主题-1"><a href="#消费者订阅主题-1" class="headerlink" title="消费者订阅主题"></a>消费者订阅主题</h4><p>这里的核心类是 <code>clients</code> 模块下的 KafkaConsumer 类，最核心的是 <code>subscribe()</code> 方法，如下所示：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener)</span> </span>{</span><br><span class="line">    acquireAndEnsureOpen();</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        maybeThrowInvalidGroupIdException();</span><br><span class="line">        <span class="keyword">if</span> (topics == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Topic collection to subscribe to cannot be null"</span>);</span><br><span class="line">        <span class="keyword">if</span> (topics.isEmpty()) {</span><br><span class="line">            <span class="comment">// treat subscribing to empty topic list as the same as unsubscribing</span></span><br><span class="line">            <span class="keyword">this</span>.unsubscribe();</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="keyword">for</span> (String topic : topics) {</span><br><span class="line">                <span class="keyword">if</span> (Utils.isBlank(topic))</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Topic collection to subscribe to cannot contain null or empty topic"</span>);</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            throwIfNoAssignorsConfigured();</span><br><span class="line">            <span class="comment">// 清空订阅异常主题的缓存数据</span></span><br><span class="line">            fetcher.clearBufferedDataForUnassignedTopics(topics);</span><br><span class="line">            log.info(<span class="string">"Subscribed to topic(s): {}"</span>, Utils.join(topics, <span class="string">", "</span>));</span><br><span class="line">            <span class="comment">// 判断是否需要更改订阅的主题，如果需要更改主题，则更新元数据信息</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">this</span>.subscriptions.subscribe(<span class="keyword">new</span> HashSet&lt;&gt;(topics), listener))</span><br><span class="line">                metadata.requestUpdateForNewTopics();</span><br><span class="line">        }</span><br><span class="line">    } <span class="keyword">finally</span> {</span><br><span class="line">        release();</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>SubscriptionState 的 <code>subscribe()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">subscribe</span><span class="params">(Set&lt;String&gt; topics, ConsumerRebalanceListener listener)</span> </span>{</span><br><span class="line">    <span class="comment">// 注册负载均衡监听（比如，在消费者组中，其他消费者退出或挂掉，触发了分区再平衡）</span></span><br><span class="line">    registerRebalanceListener(listener);</span><br><span class="line">    <span class="comment">// 按照设置的主题开始订阅，自动分配分区</span></span><br><span class="line">    setSubscriptionType(SubscriptionType.AUTO_TOPICS);</span><br><span class="line">    <span class="comment">// 更改订阅的主题</span></span><br><span class="line">    <span class="keyword">return</span> changeSubscription(topics);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>SubscriptionState 的 <code>changeSubscription()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">changeSubscription</span><span class="params">(Set&lt;String&gt; topicsToSubscribe)</span> </span>{</span><br><span class="line">    <span class="comment">// 如果订阅的主题和以前订阅的一致，就不需要更改订阅信息。如果不一致，就需要更改。</span></span><br><span class="line">    <span class="keyword">if</span> (subscription.equals(topicsToSubscribe))</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    subscription = topicsToSubscribe;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>Metadata 类的 <code>requestUpdateForNewTopics()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果订阅的主题和以前订阅的不一致，就需要更新元数据信息</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">int</span> <span class="title">requestUpdateForNewTopics</span><span class="params">()</span> </span>{</span><br><span class="line">    <span class="comment">// Override the timestamp of last refresh to let immediate update.</span></span><br><span class="line">    <span class="keyword">this</span>.lastRefreshMs = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">this</span>.needPartialUpdate = <span class="keyword">true</span>;</span><br><span class="line">    <span class="keyword">this</span>.requestVersion++;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.updateVersion;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h4 id="消费者拉取和处理消息-1"><a href="#消费者拉取和处理消息-1" class="headerlink" title="消费者拉取和处理消息"></a>消费者拉取和处理消息</h4><h5 id="消费者拉取消息"><a href="#消费者拉取消息" class="headerlink" title="消费者拉取消息"></a>消费者拉取消息</h5><p>这里的核心类是 <code>clients</code> 模块下的 KafkaConsumer 类，最核心的是 <code>poll()</code> 方法，如下所示：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ConsumerRecords&lt;K, V&gt; <span class="title">poll</span><span class="params">(<span class="keyword">final</span> Duration timeout)</span> </span>{</span><br><span class="line">    <span class="keyword">return</span> poll(time.timer(timeout), <span class="keyword">true</span>);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> ConsumerRecords&lt;K, V&gt; <span class="title">poll</span><span class="params">(<span class="keyword">final</span> Timer timer, <span class="keyword">final</span> <span class="keyword">boolean</span> includeMetadataInTimeout)</span> </span>{</span><br><span class="line">    acquireAndEnsureOpen();</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="comment">// 记录开始拉取消息的时间</span></span><br><span class="line">        <span class="keyword">this</span>.kafkaConsumerMetrics.recordPollStart(timer.currentTimeMs());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.subscriptions.hasNoSubscriptionOrUserAssignment()) {</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Consumer is not subscribed to any topics or assigned any partitions"</span>);</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="keyword">do</span> {</span><br><span class="line">            client.maybeTriggerWakeup();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (includeMetadataInTimeout) {</span><br><span class="line">                <span class="comment">// try to update assignment metadata BUT do not need to block on the timer for join group</span></span><br><span class="line">                <span class="comment">// 消费者或消费者组初始化</span></span><br><span class="line">                updateAssignmentMetadataIfNeeded(timer, <span class="keyword">false</span>);</span><br><span class="line">            } <span class="keyword">else</span> {</span><br><span class="line">                <span class="keyword">while</span> (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE), <span class="keyword">true</span>)) {</span><br><span class="line">                    log.warn(<span class="string">"Still waiting for metadata"</span>);</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 开始拉取消息</span></span><br><span class="line">            <span class="keyword">final</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollForFetches(timer);</span><br><span class="line">            <span class="keyword">if</span> (!records.isEmpty()) {</span><br><span class="line">                <span class="comment">// before returning the fetched records, we can send off the next round of fetches</span></span><br><span class="line">                <span class="comment">// and avoid block waiting for their responses to enable pipelining while the user</span></span><br><span class="line">                <span class="comment">// is handling the fetched records.</span></span><br><span class="line">                <span class="comment">//</span></span><br><span class="line">                <span class="comment">// <span class="doctag">NOTE:</span> since the consumed position has already been updated, we must not allow</span></span><br><span class="line">                <span class="comment">// wakeups or any other errors to be triggered prior to returning the fetched records.</span></span><br><span class="line">                <span class="keyword">if</span> (fetcher.sendFetches() &gt; <span class="number">0</span> || client.hasPendingRequests()) {</span><br><span class="line">                    client.transmitSends();</span><br><span class="line">                }</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 拦截器处理拉取到的消息</span></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">this</span>.interceptors.onConsume(<span class="keyword">new</span> ConsumerRecords&lt;&gt;(records));</span><br><span class="line">            }</span><br><span class="line">        } <span class="keyword">while</span> (timer.notExpired());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ConsumerRecords.empty();</span><br><span class="line">    } <span class="keyword">finally</span> {</span><br><span class="line">        release();</span><br><span class="line">        <span class="keyword">this</span>.kafkaConsumerMetrics.recordPollEnd(timer.currentTimeMs());</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>KafkaConsumer 类的 <code>pollForFetches()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; pollForFetches(Timer timer) {</span><br><span class="line">    <span class="keyword">long</span> pollTimeout = coordinator == <span class="keyword">null</span> ? timer.remainingMs() :</span><br><span class="line">            Math.min(coordinator.timeToNextPoll(timer.currentTimeMs()), timer.remainingMs());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if data is available already, return it immediately</span></span><br><span class="line">    <span class="comment">// 首先尝试在本地的队列（内存）拉取数据，如果有数据存在，就直接返回</span></span><br><span class="line">    <span class="keyword">final</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = fetcher.fetchedRecords();</span><br><span class="line">    <span class="keyword">if</span> (!records.isEmpty()) {</span><br><span class="line">        <span class="keyword">return</span> records;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// send any new fetches (won't resend pending fetches)</span></span><br><span class="line">    <span class="comment">// 发送请求从服务端抓取数据（一次最多抓取 50m 数据）</span></span><br><span class="line">    fetcher.sendFetches();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// We do not want to be stuck blocking in poll if we are missing some positions</span></span><br><span class="line">    <span class="comment">// since the offset lookup may be backing off after a failure</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// <span class="doctag">NOTE:</span> the use of cachedSubscriptionHashAllFetchPositions means we MUST call</span></span><br><span class="line">    <span class="comment">// updateAssignmentMetadataIfNeeded before this method.</span></span><br><span class="line">    <span class="keyword">if</span> (!cachedSubscriptionHashAllFetchPositions &amp;&amp; pollTimeout &gt; retryBackoffMs) {</span><br><span class="line">        pollTimeout = retryBackoffMs;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    log.trace(<span class="string">"Polling for fetches with timeout {}"</span>, pollTimeout);</span><br><span class="line"></span><br><span class="line">    Timer pollTimer = time.timer(pollTimeout);</span><br><span class="line">    client.poll(pollTimer, () -&gt; {</span><br><span class="line">        <span class="comment">// since a fetch might be completed by the background thread, we need this poll condition</span></span><br><span class="line">        <span class="comment">// to ensure that we do not block unnecessarily in poll()</span></span><br><span class="line">        <span class="keyword">return</span> !fetcher.hasAvailableFetches();</span><br><span class="line">    });</span><br><span class="line">    timer.update(pollTimer.currentTimeMs());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从本地的队列（内存）拉取数据，一次最多拉取 500 条</span></span><br><span class="line">    <span class="keyword">return</span> fetcher.fetchedRecords();</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>Fetcher 类的 <code>sendFetches()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">int</span> <span class="title">sendFetches</span><span class="params">()</span> </span>{</span><br><span class="line">    <span class="comment">// Update metrics in case there was an assignment change</span></span><br><span class="line">    sensors.maybeUpdateAssignment(subscriptions);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化抓取数据的配置参数</span></span><br><span class="line">    Map&lt;Node, FetchSessionHandler.FetchRequestData&gt; fetchRequestMap = prepareFetchRequests();</span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;Node, FetchSessionHandler.FetchRequestData&gt; entry : fetchRequestMap.entrySet()) {</span><br><span class="line">        <span class="keyword">final</span> Node fetchTarget = entry.getKey();</span><br><span class="line">        <span class="keyword">final</span> FetchSessionHandler.FetchRequestData data = entry.getValue();</span><br><span class="line">        <span class="keyword">final</span> FetchRequest.Builder request = FetchRequest.Builder</span><br><span class="line">                <span class="comment">// 最大等待时间，默认值是 500ms</span></span><br><span class="line">                <span class="comment">// 最小抓取的字节数，默认值是 1 个字节</span></span><br><span class="line">                .forConsumer(<span class="keyword">this</span>.maxWaitMs, <span class="keyword">this</span>.minBytes, data.toSend())</span><br><span class="line">                .isolationLevel(isolationLevel)</span><br><span class="line">                <span class="comment">// 最大抓取的字节数，默认值是 50m</span></span><br><span class="line">                .setMaxBytes(<span class="keyword">this</span>.maxBytes)</span><br><span class="line">                .metadata(data.metadata())</span><br><span class="line">                .toForget(data.toForget())</span><br><span class="line">                .rackId(clientRackId);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (log.isDebugEnabled()) {</span><br><span class="line">            log.debug(<span class="string">"Sending {} {} to broker {}"</span>, isolationLevel, data.toString(), fetchTarget);</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 发送拉取数据的请求</span></span><br><span class="line">        RequestFuture&lt;ClientResponse&gt; future = client.send(fetchTarget, request);</span><br><span class="line">        <span class="comment">// We add the node to the set of nodes with pending fetch requests before adding the</span></span><br><span class="line">        <span class="comment">// listener because the future may have been fulfilled on another thread (e.g. during a</span></span><br><span class="line">        <span class="comment">// disconnection being handled by the heartbeat thread) which will mean the listener</span></span><br><span class="line">        <span class="comment">// will be invoked synchronously.</span></span><br><span class="line">        <span class="keyword">this</span>.nodesWithPendingFetchRequests.add(entry.getKey().id());</span><br><span class="line">        <span class="comment">// 监听服务端返回数据</span></span><br><span class="line">        future.addListener(<span class="keyword">new</span> RequestFutureListener&lt;ClientResponse&gt;() {</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 成功接收服务端返回的数据</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(ClientResponse resp)</span> </span>{</span><br><span class="line">                <span class="keyword">synchronized</span> (Fetcher.<span class="keyword">this</span>) {</span><br><span class="line">                    <span class="keyword">try</span> {</span><br><span class="line">                        <span class="comment">// 获取服务端返回的数据</span></span><br><span class="line">                        FetchResponse response = (FetchResponse) resp.responseBody();</span><br><span class="line">                        FetchSessionHandler handler = sessionHandler(fetchTarget.id());</span><br><span class="line">                        <span class="keyword">if</span> (handler == <span class="keyword">null</span>) {</span><br><span class="line">                            log.error(<span class="string">"Unable to find FetchSessionHandler for node {}. Ignoring fetch response."</span>,</span><br><span class="line">                                    fetchTarget.id());</span><br><span class="line">                            <span class="keyword">return</span>;</span><br><span class="line">                        }</span><br><span class="line">                        <span class="keyword">if</span> (!handler.handleResponse(response)) {</span><br><span class="line">                            <span class="keyword">return</span>;</span><br><span class="line">                        }</span><br><span class="line"></span><br><span class="line">                        Set&lt;TopicPartition&gt; partitions = <span class="keyword">new</span> HashSet&lt;&gt;(response.responseData().keySet());</span><br><span class="line">                        FetchResponseMetricAggregator metricAggregator = <span class="keyword">new</span> FetchResponseMetricAggregator(sensors, partitions);</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">for</span> (Map.Entry&lt;TopicPartition, FetchResponseData.PartitionData&gt; entry : response.responseData().entrySet()) {</span><br><span class="line">                            TopicPartition partition = entry.getKey();</span><br><span class="line">                            FetchRequest.PartitionData requestData = data.sessionPartitions().get(partition);</span><br><span class="line">                            <span class="keyword">if</span> (requestData == <span class="keyword">null</span>) {</span><br><span class="line">                                String message;</span><br><span class="line">                                <span class="keyword">if</span> (data.metadata().isFull()) {</span><br><span class="line">                                    message = MessageFormatter.arrayFormat(</span><br><span class="line">                                            <span class="string">"Response for missing full request partition: partition={}; metadata={}"</span>,</span><br><span class="line">                                            <span class="keyword">new</span> Object[]{partition, data.metadata()}).getMessage();</span><br><span class="line">                                } <span class="keyword">else</span> {</span><br><span class="line">                                    message = MessageFormatter.arrayFormat(</span><br><span class="line">                                            <span class="string">"Response for missing session request partition: partition={}; metadata={}; toSend={}; toForget={}"</span>,</span><br><span class="line">                                            <span class="keyword">new</span> Object[]{partition, data.metadata(), data.toSend(), data.toForget()}).getMessage();</span><br><span class="line">                                }</span><br><span class="line"></span><br><span class="line">                                <span class="comment">// Received fetch response for missing session partition</span></span><br><span class="line">                                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(message);</span><br><span class="line">                            } <span class="keyword">else</span> {</span><br><span class="line">                                <span class="keyword">long</span> fetchOffset = requestData.fetchOffset;</span><br><span class="line">                                FetchResponseData.PartitionData partitionData = entry.getValue();</span><br><span class="line"></span><br><span class="line">                                log.debug(<span class="string">"Fetch {} at offset {} for partition {} returned fetch data {}"</span>,</span><br><span class="line">                                        isolationLevel, fetchOffset, partition, partitionData);</span><br><span class="line"></span><br><span class="line">                                Iterator&lt;? extends RecordBatch&gt; batches = FetchResponse.recordsOrFail(partitionData).batches().iterator();</span><br><span class="line">                                <span class="keyword">short</span> responseVersion = resp.requestHeader().apiVersion();</span><br><span class="line"></span><br><span class="line">                                <span class="comment">// 将从服务端抓取到的数据按照分区，添加到本地的队列（内存）里面</span></span><br><span class="line">                                completedFetches.add(<span class="keyword">new</span> CompletedFetch(partition, partitionData,</span><br><span class="line">                                        metricAggregator, batches, fetchOffset, responseVersion));</span><br><span class="line">                            }</span><br><span class="line">                        }</span><br><span class="line"></span><br><span class="line">                        sensors.fetchLatency.record(resp.requestLatencyMs());</span><br><span class="line">                    } <span class="keyword">finally</span> {</span><br><span class="line">                        nodesWithPendingFetchRequests.remove(fetchTarget.id());</span><br><span class="line">                    }</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e)</span> </span>{</span><br><span class="line">                <span class="keyword">synchronized</span> (Fetcher.<span class="keyword">this</span>) {</span><br><span class="line">                    <span class="keyword">try</span> {</span><br><span class="line">                        FetchSessionHandler handler = sessionHandler(fetchTarget.id());</span><br><span class="line">                        <span class="keyword">if</span> (handler != <span class="keyword">null</span>) {</span><br><span class="line">                            handler.handleError(e);</span><br><span class="line">                        }</span><br><span class="line">                    } <span class="keyword">finally</span> {</span><br><span class="line">                        nodesWithPendingFetchRequests.remove(fetchTarget.id());</span><br><span class="line">                    }</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">        });</span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> fetchRequestMap.size();</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>Fetcher 类的 <code>fetchedRecords()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从本地的队列（内存）里拉取数据，一次默认最多拉取 500 条数据</span></span><br><span class="line"><span class="keyword">public</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; fetchedRecords() {</span><br><span class="line">    Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; fetched = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    Queue&lt;CompletedFetch&gt; pausedCompletedFetches = <span class="keyword">new</span> ArrayDeque&lt;&gt;();</span><br><span class="line">    <span class="comment">// 一次拉取数据的最大条数，默认值是 500 条</span></span><br><span class="line">    <span class="keyword">int</span> recordsRemaining = maxPollRecords;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        <span class="comment">// 循环处理</span></span><br><span class="line">        <span class="keyword">while</span> (recordsRemaining &gt; <span class="number">0</span>) {</span><br><span class="line">            <span class="keyword">if</span> (nextInLineFetch == <span class="keyword">null</span> || nextInLineFetch.isConsumed) {</span><br><span class="line">                <span class="comment">// 从本地的队列（内存）里拉取数据</span></span><br><span class="line">                <span class="comment">// peek() 方法：获取队列的头元素，但不会移除它，如果队列为空，则返回 null，而不会抛出异常</span></span><br><span class="line">                CompletedFetch records = completedFetches.peek();</span><br><span class="line">                <span class="comment">// 如果本地的队列（内存）里没有数据，直接跳出循环</span></span><br><span class="line">                <span class="keyword">if</span> (records == <span class="keyword">null</span>) <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (records.notInitialized()) {</span><br><span class="line">                    <span class="keyword">try</span> {</span><br><span class="line">                        nextInLineFetch = initializeCompletedFetch(records);</span><br><span class="line">                    } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">                        <span class="comment">// Remove a completedFetch upon a parse with exception if (1) it contains no records, and</span></span><br><span class="line">                        <span class="comment">// (2) there are no fetched records with actual content preceding this exception.</span></span><br><span class="line">                        <span class="comment">// The first condition ensures that the completedFetches is not stuck with the same completedFetch</span></span><br><span class="line">                        <span class="comment">// in cases such as the TopicAuthorizationException, and the second condition ensures that no</span></span><br><span class="line">                        <span class="comment">// potential data loss due to an exception in a following record.</span></span><br><span class="line">                        FetchResponseData.PartitionData partition = records.partitionData;</span><br><span class="line">                        <span class="keyword">if</span> (fetched.isEmpty() &amp;&amp; FetchResponse.recordsOrFail(partition).sizeInBytes() == <span class="number">0</span>) {</span><br><span class="line">                            completedFetches.poll();</span><br><span class="line">                        }</span><br><span class="line">                        <span class="keyword">throw</span> e;</span><br><span class="line">                    }</span><br><span class="line">                } <span class="keyword">else</span> {</span><br><span class="line">                    nextInLineFetch = records;</span><br><span class="line">                }</span><br><span class="line">                <span class="comment">// 从本地的队列（内存）里移除数据</span></span><br><span class="line">                <span class="comment">// poll() 方法：移除并返回队头元素，如果队列为空返回 null，并且不会抛出异常</span></span><br><span class="line">                completedFetches.poll();</span><br><span class="line">            } <span class="keyword">else</span> <span class="keyword">if</span> (subscriptions.isPaused(nextInLineFetch.partition)) {</span><br><span class="line">                <span class="comment">// when the partition is paused we add the records back to the completedFetches queue instead of draining</span></span><br><span class="line">                <span class="comment">// them so that they can be returned on a subsequent poll if the partition is resumed at that time</span></span><br><span class="line">                log.debug(<span class="string">"Skipping fetching records for assigned partition {} because it is paused"</span>, nextInLineFetch.partition);</span><br><span class="line">                pausedCompletedFetches.add(nextInLineFetch);</span><br><span class="line">                nextInLineFetch = <span class="keyword">null</span>;</span><br><span class="line">            } <span class="keyword">else</span> {</span><br><span class="line">                List&lt;ConsumerRecord&lt;K, V&gt;&gt; records = fetchRecords(nextInLineFetch, recordsRemaining);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (!records.isEmpty()) {</span><br><span class="line">                    TopicPartition partition = nextInLineFetch.partition;</span><br><span class="line">                    List&lt;ConsumerRecord&lt;K, V&gt;&gt; currentRecords = fetched.get(partition);</span><br><span class="line">                    <span class="keyword">if</span> (currentRecords == <span class="keyword">null</span>) {</span><br><span class="line">                        fetched.put(partition, records);</span><br><span class="line">                    } <span class="keyword">else</span> {</span><br><span class="line">                        <span class="comment">// this case shouldn't usually happen because we only send one fetch at a time per partition,</span></span><br><span class="line">                        <span class="comment">// but it might conceivably happen in some rare cases (such as partition leader changes).</span></span><br><span class="line">                        <span class="comment">// we have to copy to a new list because the old one may be immutable</span></span><br><span class="line">                        List&lt;ConsumerRecord&lt;K, V&gt;&gt; newRecords = <span class="keyword">new</span> ArrayList&lt;&gt;(records.size() + currentRecords.size());</span><br><span class="line">                        newRecords.addAll(currentRecords);</span><br><span class="line">                        newRecords.addAll(records);</span><br><span class="line">                        fetched.put(partition, newRecords);</span><br><span class="line">                    }</span><br><span class="line">                    recordsRemaining -= records.size();</span><br><span class="line">                }</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    } <span class="keyword">catch</span> (KafkaException e) {</span><br><span class="line">        <span class="keyword">if</span> (fetched.isEmpty())</span><br><span class="line">            <span class="keyword">throw</span> e;</span><br><span class="line">    } <span class="keyword">finally</span> {</span><br><span class="line">        <span class="comment">// add any polled completed fetches for paused partitions back to the completed fetches queue to be</span></span><br><span class="line">        <span class="comment">// re-evaluated in the next poll</span></span><br><span class="line">        completedFetches.addAll(pausedCompletedFetches);</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> fetched;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h5 id="消费者处理消息"><a href="#消费者处理消息" class="headerlink" title="消费者处理消息"></a>消费者处理消息</h5><p>在 KafkaConsumer 中的 <code>poll()</code> 方法中，有以下一段代码：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开始拉取数据</span></span><br><span class="line"><span class="keyword">final</span> Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollForFetches(timer);</span><br><span class="line"><span class="keyword">if</span> (!records.isEmpty()) {</span><br><span class="line">    <span class="comment">// before returning the fetched records, we can send off the next round of fetches</span></span><br><span class="line">    <span class="comment">// and avoid block waiting for their responses to enable pipelining while the user</span></span><br><span class="line">    <span class="comment">// is handling the fetched records.</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// <span class="doctag">NOTE:</span> since the consumed position has already been updated, we must not allow</span></span><br><span class="line">    <span class="comment">// wakeups or any other errors to be triggered prior to returning the fetched records.</span></span><br><span class="line">    <span class="keyword">if</span> (fetcher.sendFetches() &gt; <span class="number">0</span> || client.hasPendingRequests()) {</span><br><span class="line">        client.transmitSends();</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拦截器处理拉取得到的数据</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.interceptors.onConsume(<span class="keyword">new</span> ConsumerRecords&lt;&gt;(records));</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>ConsumerInterceptors 的 <code>onConsume()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> ConsumerRecords&lt;K, V&gt; <span class="title">onConsume</span><span class="params">(ConsumerRecords&lt;K, V&gt; records)</span> </span>{</span><br><span class="line">    ConsumerRecords&lt;K, V&gt; interceptRecords = records;</span><br><span class="line">    <span class="keyword">for</span> (ConsumerInterceptor&lt;K, V&gt; interceptor : <span class="keyword">this</span>.interceptors) {</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            <span class="comment">// 拦截器处理拉取得到的数据</span></span><br><span class="line">            interceptRecords = interceptor.onConsume(interceptRecords);</span><br><span class="line">        } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">            <span class="comment">// do not propagate interceptor exception, log and continue calling other interceptors</span></span><br><span class="line">            log.warn(<span class="string">"Error executing interceptor onConsume callback"</span>, e);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> interceptRecords;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h4 id="消费者提交-Offset-1"><a href="#消费者提交-Offset-1" class="headerlink" title="消费者提交 Offset"></a>消费者提交 Offset</h4><h5 id="手动同步提交-Offset"><a href="#手动同步提交-Offset" class="headerlink" title="手动同步提交 Offset"></a>手动同步提交 Offset</h5><p>这里的核心类是 <code>clients</code> 模块下的 KafkaConsumer 类，最核心的是 <code>commitSync()</code> 方法，如下所示：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitSync</span><span class="params">()</span> </span>{</span><br><span class="line">    commitSync(Duration.ofMillis(defaultApiTimeoutMs));</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitSync</span><span class="params">(Duration timeout)</span> </span>{</span><br><span class="line">    commitSync(subscriptions.allConsumed(), timeout);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitSync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, <span class="keyword">final</span> Duration timeout)</span> </span>{</span><br><span class="line">    acquireAndEnsureOpen();</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        maybeThrowInvalidGroupIdException();</span><br><span class="line">        offsets.forEach(<span class="keyword">this</span>::updateLastSeenEpochIfNewer);</span><br><span class="line">        <span class="comment">// 同步提交 Offset</span></span><br><span class="line">        <span class="keyword">if</span> (!coordinator.commitOffsetsSync(<span class="keyword">new</span> HashMap&lt;&gt;(offsets), time.timer(timeout))) {</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> TimeoutException(<span class="string">"Timeout of "</span> + timeout.toMillis() + <span class="string">"ms expired before successfully "</span> +</span><br><span class="line">                    <span class="string">"committing offsets "</span> + offsets);</span><br><span class="line">        }</span><br><span class="line">    } <span class="keyword">finally</span> {</span><br><span class="line">        release();</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>ConsumerCoordinator 的 <code>commitOffsetsSync()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 同步提交 Offset</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">commitOffsetsSync</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Timer timer)</span> </span>{</span><br><span class="line">    invokeCompletedOffsetCommitCallbacks();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (offsets.isEmpty())</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 循环处理</span></span><br><span class="line">    <span class="keyword">do</span> {</span><br><span class="line">        <span class="keyword">if</span> (coordinatorUnknown() &amp;&amp; !ensureCoordinatorReady(timer)) {</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 发送提交 Offset 的请求</span></span><br><span class="line">        RequestFuture&lt;Void&gt; future = sendOffsetCommitRequest(offsets);</span><br><span class="line">        <span class="comment">// 等待发送提交 Offset 请求的响应结果</span></span><br><span class="line">        client.poll(future, timer);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// We may have had in-flight offset commits when the synchronous commit began. If so, ensure that</span></span><br><span class="line">        <span class="comment">// the corresponding callbacks are invoked prior to returning in order to preserve the order that</span></span><br><span class="line">        <span class="comment">// the offset commits were applied.</span></span><br><span class="line">        invokeCompletedOffsetCommitCallbacks();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交成功</span></span><br><span class="line">        <span class="keyword">if</span> (future.succeeded()) {</span><br><span class="line">            <span class="keyword">if</span> (interceptors != <span class="keyword">null</span>)</span><br><span class="line">                interceptors.onCommit(offsets);</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (future.failed() &amp;&amp; !future.isRetriable())</span><br><span class="line">            <span class="keyword">throw</span> future.exception();</span><br><span class="line"></span><br><span class="line">        timer.sleep(rebalanceConfig.retryBackoffMs);</span><br><span class="line">    } <span class="keyword">while</span> (timer.notExpired());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h5 id="手动异步提交-Offset"><a href="#手动异步提交-Offset" class="headerlink" title="手动异步提交 Offset"></a>手动异步提交 Offset</h5><p>这里的核心类是 <code>clients</code> 模块下的 KafkaConsumer 类，最核心的是 <code>commitAsync()</code> 方法，如下所示：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitAsync</span><span class="params">()</span> </span>{</span><br><span class="line">    commitAsync(<span class="keyword">null</span>);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitAsync</span><span class="params">(OffsetCommitCallback callback)</span> </span>{</span><br><span class="line">    commitAsync(subscriptions.allConsumed(), callback);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitAsync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback)</span> </span>{</span><br><span class="line">    acquireAndEnsureOpen();</span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">        maybeThrowInvalidGroupIdException();</span><br><span class="line">        log.debug(<span class="string">"Committing offsets: {}"</span>, offsets);</span><br><span class="line">        offsets.forEach(<span class="keyword">this</span>::updateLastSeenEpochIfNewer);</span><br><span class="line">        <span class="comment">// 异步提交 Offset</span></span><br><span class="line">        coordinator.commitOffsetsAsync(<span class="keyword">new</span> HashMap&lt;&gt;(offsets), callback);</span><br><span class="line">    } <span class="keyword">finally</span> {</span><br><span class="line">        release();</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>ConsumerCoordinator 的 <code>commitOffsetsAsync()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitOffsetsAsync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, <span class="keyword">final</span> OffsetCommitCallback callback)</span> </span>{</span><br><span class="line">    invokeCompletedOffsetCommitCallbacks();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 检测 Coordinator 是否存在</span></span><br><span class="line">    <span class="keyword">if</span> (!coordinatorUnknown()) {</span><br><span class="line">        <span class="comment">// 如果 Coordinator 存在，异步提交 Offset</span></span><br><span class="line">        doCommitOffsetsAsync(offsets, callback);</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        <span class="comment">// we don't know the current coordinator, so try to find it and then send the commit</span></span><br><span class="line">        <span class="comment">// or fail (we don't want recursive retries which can cause offset commits to arrive</span></span><br><span class="line">        <span class="comment">// out of order). Note that there may be multiple offset commits chained to the same</span></span><br><span class="line">        <span class="comment">// coordinator lookup request. This is fine because the listeners will be invoked in</span></span><br><span class="line">        <span class="comment">// the same order that they were added. Note also that AbstractCoordinator prevents</span></span><br><span class="line">        <span class="comment">// multiple concurrent coordinator lookup requests.</span></span><br><span class="line">        pendingAsyncCommits.incrementAndGet();</span><br><span class="line">        <span class="comment">// 如果 Coordinator 不存在，寻找 Coordinator，并添加监听器</span></span><br><span class="line">        lookupCoordinator().addListener(<span class="keyword">new</span> RequestFutureListener&lt;Void&gt;() {</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(Void value)</span> </span>{</span><br><span class="line">                pendingAsyncCommits.decrementAndGet();</span><br><span class="line">                <span class="comment">// 寻找到 Coordinator 后，异步提交 Offset</span></span><br><span class="line">                doCommitOffsetsAsync(offsets, callback);</span><br><span class="line">                client.pollNoWakeup();</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e)</span> </span>{</span><br><span class="line">                pendingAsyncCommits.decrementAndGet();</span><br><span class="line">                completedOffsetCommits.add(<span class="keyword">new</span> OffsetCommitCompletion(callback, offsets,</span><br><span class="line">                        <span class="keyword">new</span> RetriableCommitFailedException(e)));</span><br><span class="line">            }</span><br><span class="line">        });</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ensure the commit has a chance to be transmitted (without blocking on its completion).</span></span><br><span class="line">    <span class="comment">// Note that commits are treated as heartbeats by the coordinator, so there is no need to</span></span><br><span class="line">    <span class="comment">// explicitly allow heartbeats through delayed task execution.</span></span><br><span class="line">    client.pollNoWakeup();</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doCommitOffsetsAsync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, <span class="keyword">final</span> OffsetCommitCallback callback)</span> </span>{</span><br><span class="line">    <span class="comment">// 发送提交 Offset 的请求</span></span><br><span class="line">    RequestFuture&lt;Void&gt; future = sendOffsetCommitRequest(offsets);</span><br><span class="line">    <span class="keyword">final</span> OffsetCommitCallback cb = callback == <span class="keyword">null</span> ? defaultOffsetCommitCallback : callback;</span><br><span class="line">    <span class="comment">// 监听提交 Offset 的结果</span></span><br><span class="line">    future.addListener(<span class="keyword">new</span> RequestFutureListener&lt;Void&gt;() {</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(Void value)</span> </span>{</span><br><span class="line">            <span class="comment">// 提交 Offset 成功</span></span><br><span class="line">            <span class="keyword">if</span> (interceptors != <span class="keyword">null</span>)</span><br><span class="line">                interceptors.onCommit(offsets);</span><br><span class="line">            completedOffsetCommits.add(<span class="keyword">new</span> OffsetCommitCompletion(cb, offsets, <span class="keyword">null</span>));</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(RuntimeException e)</span> </span>{</span><br><span class="line">            Exception commitException = e;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (e <span class="keyword">instanceof</span> RetriableException) {</span><br><span class="line">                commitException = <span class="keyword">new</span> RetriableCommitFailedException(e);</span><br><span class="line">            }</span><br><span class="line">            completedOffsetCommits.add(<span class="keyword">new</span> OffsetCommitCompletion(cb, offsets, commitException));</span><br><span class="line">            <span class="keyword">if</span> (commitException <span class="keyword">instanceof</span> FencedInstanceIdException) {</span><br><span class="line">                asyncCommitFenced.set(<span class="keyword">true</span>);</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    });</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h4 id="消费者组的初始化流程-1"><a href="#消费者组的初始化流程-1" class="headerlink" title="消费者组的初始化流程"></a>消费者组的初始化流程</h4><p>这里的核心类是 <code>clients</code> 模块下的 KafkaConsumer 类，最核心的是 <code>updateAssignmentMetadataIfNeeded()</code> 方法，如下所示：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">updateAssignmentMetadataIfNeeded</span><span class="params">(<span class="keyword">final</span> Timer timer, <span class="keyword">final</span> <span class="keyword">boolean</span> waitForJoinGroup)</span> </span>{</span><br><span class="line">    <span class="keyword">if</span> (coordinator != <span class="keyword">null</span> &amp;&amp; !coordinator.poll(timer, waitForJoinGroup)) {</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> updateFetchPositions(timer);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>ConsumerCoordinator 的 <code>poll()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">poll</span><span class="params">(Timer timer, <span class="keyword">boolean</span> waitForJoinGroup)</span> </span>{</span><br><span class="line">    <span class="comment">// 获取最新的元数据</span></span><br><span class="line">    maybeUpdateSubscriptionMetadata();</span><br><span class="line"></span><br><span class="line">    invokeCompletedOffsetCommitCallbacks();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (subscriptions.hasAutoAssignedPartitions()) {</span><br><span class="line">        <span class="keyword">if</span> (protocol == <span class="keyword">null</span>) {</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"User configured "</span> + ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG +</span><br><span class="line">                <span class="string">" to empty while trying to subscribe for group protocol to auto assign partitions"</span>);</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// Always update the heartbeat last poll time so that the heartbeat thread does not leave the</span></span><br><span class="line">        <span class="comment">// group proactively due to application inactivity even if (say) the coordinator cannot be found.</span></span><br><span class="line">        <span class="comment">// 发送心跳给 Coordinator（每隔 3s 发送一次）</span></span><br><span class="line">        pollHeartbeat(timer.currentTimeMs());</span><br><span class="line">        <span class="comment">// 保证消费者客户端和 Coordinator 可以正常通信（寻找服务端的 Coordinator）</span></span><br><span class="line">        <span class="keyword">if</span> (coordinatorUnknown() &amp;&amp; !ensureCoordinatorReady(timer)) {</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断是否需要加入消费者组</span></span><br><span class="line">        <span class="keyword">if</span> (rejoinNeededOrPending()) {</span><br><span class="line">            <span class="comment">// due to a race condition between the initial metadata fetch and the initial rebalance,</span></span><br><span class="line">            <span class="comment">// we need to ensure that the metadata is fresh before joining initially. This ensures</span></span><br><span class="line">            <span class="comment">// that we have matched the pattern against the cluster's topics at least once before joining.</span></span><br><span class="line">            <span class="keyword">if</span> (subscriptions.hasPatternSubscription()) {</span><br><span class="line">                <span class="comment">// For consumer group that uses pattern-based subscription, after a topic is created,</span></span><br><span class="line">                <span class="comment">// any consumer that discovers the topic after metadata refresh can trigger rebalance</span></span><br><span class="line">                <span class="comment">// across the entire consumer group. Multiple rebalances can be triggered after one topic</span></span><br><span class="line">                <span class="comment">// creation if consumers refresh metadata at vastly different times. We can significantly</span></span><br><span class="line">                <span class="comment">// reduce the number of rebalances caused by single topic creation by asking consumer to</span></span><br><span class="line">                <span class="comment">// refresh metadata before re-joining the group as long as the refresh backoff time has</span></span><br><span class="line">                <span class="comment">// passed.</span></span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">this</span>.metadata.timeToAllowUpdate(timer.currentTimeMs()) == <span class="number">0</span>) {</span><br><span class="line">                    <span class="keyword">this</span>.metadata.requestUpdate();</span><br><span class="line">                }</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (!client.ensureFreshMetadata(timer)) {</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">                }</span><br><span class="line"></span><br><span class="line">                maybeUpdateSubscriptionMetadata();</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">            <span class="comment">// if not wait for join group, we would just use a timer of 0</span></span><br><span class="line">            <span class="keyword">if</span> (!ensureActiveGroup(waitForJoinGroup ? timer : time.timer(<span class="number">0L</span>))) {</span><br><span class="line">                <span class="comment">// since we may use a different timer in the callee, we'd still need</span></span><br><span class="line">                <span class="comment">// to update the original timer's current time after the call</span></span><br><span class="line">                timer.update(time.milliseconds());</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        <span class="comment">// For manually assigned partitions, if there are no ready nodes, await metadata.</span></span><br><span class="line">        <span class="comment">// If connections to all nodes fail, wakeups triggered while attempting to send fetch</span></span><br><span class="line">        <span class="comment">// requests result in polls returning immediately, causing a tight loop of polls. Without</span></span><br><span class="line">        <span class="comment">// the wakeup, poll() with no channels would block for the timeout, delaying re-connection.</span></span><br><span class="line">        <span class="comment">// awaitMetadataUpdate() initiates new connections with configured backoff and avoids the busy loop.</span></span><br><span class="line">        <span class="comment">// When group management is used, metadata wait is already performed for this scenario as</span></span><br><span class="line">        <span class="comment">// coordinator is unknown, hence this check is not required.</span></span><br><span class="line">        <span class="keyword">if</span> (metadata.updateRequested() &amp;&amp; !client.hasReadyNodes(timer.currentTimeMs())) {</span><br><span class="line">            client.awaitMetadataUpdate(timer);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 是否自动提交 Offset</span></span><br><span class="line">    maybeAutoCommitOffsetsAsync(timer.currentTimeMs());</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>AbstractCoordinator 的 <code>ensureCoordinatorReady()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">ensureCoordinatorReady</span><span class="params">(<span class="keyword">final</span> Timer timer)</span> </span>{</span><br><span class="line">    <span class="comment">// 如果已经找到 Coordinator，则直接返回</span></span><br><span class="line">    <span class="keyword">if</span> (!coordinatorUnknown())</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果没有找到 Coordinator，则循环给服务端发送请求，直到找到 Coordinator 为止</span></span><br><span class="line">    <span class="keyword">do</span> {</span><br><span class="line">        <span class="keyword">if</span> (fatalFindCoordinatorException != <span class="keyword">null</span>) {</span><br><span class="line">            <span class="keyword">final</span> RuntimeException fatalException = fatalFindCoordinatorException;</span><br><span class="line">            fatalFindCoordinatorException = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">throw</span> fatalException;</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 发送寻找 Coordinator 的请求给服务端</span></span><br><span class="line">        <span class="keyword">final</span> RequestFuture&lt;Void&gt; future = lookupCoordinator();</span><br><span class="line">        <span class="comment">// 等待发送寻找 Coordinator 请求的响应结果</span></span><br><span class="line">        client.poll(future, timer);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!future.isDone()) {</span><br><span class="line">            <span class="comment">// ran out of time</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        RuntimeException fatalException = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (future.failed()) {</span><br><span class="line">            <span class="keyword">if</span> (future.isRetriable()) {</span><br><span class="line">                log.debug(<span class="string">"Coordinator discovery failed, refreshing metadata"</span>, future.exception());</span><br><span class="line">                client.awaitMetadataUpdate(timer);</span><br><span class="line">            } <span class="keyword">else</span> {</span><br><span class="line">                fatalException = future.exception();</span><br><span class="line">                log.info(<span class="string">"FindCoordinator request hit fatal exception"</span>, fatalException);</span><br><span class="line">            }</span><br><span class="line">        } <span class="keyword">else</span> <span class="keyword">if</span> (coordinator != <span class="keyword">null</span> &amp;&amp; client.isUnavailable(coordinator)) {</span><br><span class="line">            <span class="comment">// we found the coordinator, but the connection has failed, so mark</span></span><br><span class="line">            <span class="comment">// it dead and backoff before retrying discovery</span></span><br><span class="line">            markCoordinatorUnknown(<span class="string">"coordinator unavailable"</span>);</span><br><span class="line">            timer.sleep(rebalanceConfig.retryBackoffMs);</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        clearFindCoordinatorFuture();</span><br><span class="line">        <span class="keyword">if</span> (fatalException != <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">throw</span> fatalException;</span><br><span class="line">    } <span class="keyword">while</span> (coordinatorUnknown() &amp;&amp; timer.notExpired());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> !coordinatorUnknown();</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> RequestFuture&lt;Void&gt; <span class="title">lookupCoordinator</span><span class="params">()</span> </span>{</span><br><span class="line">    <span class="keyword">if</span> (findCoordinatorFuture == <span class="keyword">null</span>) {</span><br><span class="line">        <span class="comment">// find a node to ask about the coordinator</span></span><br><span class="line">        Node node = <span class="keyword">this</span>.client.leastLoadedNode();</span><br><span class="line">        <span class="keyword">if</span> (node == <span class="keyword">null</span>) {</span><br><span class="line">            log.debug(<span class="string">"No broker available to send FindCoordinator request"</span>);</span><br><span class="line">            <span class="keyword">return</span> RequestFuture.noBrokersAvailable();</span><br><span class="line">        } <span class="keyword">else</span> {</span><br><span class="line">            <span class="comment">// 发送寻找 Coordinator 的请求给服务端</span></span><br><span class="line">            findCoordinatorFuture = sendFindCoordinatorRequest(node);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> findCoordinatorFuture;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> RequestFuture&lt;Void&gt; <span class="title">sendFindCoordinatorRequest</span><span class="params">(Node node)</span> </span>{</span><br><span class="line">    <span class="comment">// initiate the group metadata request</span></span><br><span class="line">    log.debug(<span class="string">"Sending FindCoordinator request to broker {}"</span>, node);</span><br><span class="line">    <span class="comment">// 封装发送请求的数据</span></span><br><span class="line">    FindCoordinatorRequestData data = <span class="keyword">new</span> FindCoordinatorRequestData()</span><br><span class="line">            .setKeyType(CoordinatorType.GROUP.id())</span><br><span class="line">            .setKey(<span class="keyword">this</span>.rebalanceConfig.groupId);</span><br><span class="line">    FindCoordinatorRequest.Builder requestBuilder = <span class="keyword">new</span> FindCoordinatorRequest.Builder(data);</span><br><span class="line">    <span class="comment">// 发送寻找 Coordinator 的请求给服务端</span></span><br><span class="line">    <span class="keyword">return</span> client.send(node, requestBuilder)</span><br><span class="line">            .compose(<span class="keyword">new</span> FindCoordinatorResponseHandler());</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h2 id="Kafka-服务端源码分析"><a href="#Kafka-服务端源码分析" class="headerlink" title="Kafka 服务端源码分析"></a>Kafka 服务端源码分析</h2><div class="admonition note"><p class="admonition-title">提示</p><ul><li>在阅读和调试 Kafka 的源码之前，必须先搭建 Kafka 源码的阅读环境，详细教程请看 <a href="/posts/a2880619.html">这里</a>。</li><li>验证 Kafka 源码阅读成果的两个标志：第一个是能够自行调试源码，第二个是能够在源码上独立开发高阶功能。</li><li><strong>Kafka 的服务端（Broker）主要是基于 Scala 语言开发的，因此在阅读 Kafka 服务端的源码之前，必须熟练掌握 Scala 语言。</strong></li></ul></div><h3 id="服务端的工作原理"><a href="#服务端的工作原理" class="headerlink" title="服务端的工作原理"></a>服务端的工作原理</h3><h4 id="服务端的整体工作流程"><a href="#服务端的整体工作流程" class="headerlink" title="服务端的整体工作流程"></a>服务端的整体工作流程</h4><p><img data-src="../../../asset/2024/12/kafka-code-12.png"></p><h3 id="服务端的源码分析"><a href="#服务端的源码分析" class="headerlink" title="服务端的源码分析"></a>服务端的源码分析</h3><h4 id="服务端的主启动类"><a href="#服务端的主启动类" class="headerlink" title="服务端的主启动类"></a>服务端的主启动类</h4><p>Kafka 服务端的主启动类是 <code>core/src/main/scala/kafka/Kafka.scala</code>，在 IntelliJ IDEA 中打开该主启动类，并启动 <code>main()</code> 方法即可运行服务端（前提是 Kafka 的源码阅读环境已经搭建好），如下图所示：</p><p><img data-src="../../../asset/2024/12/kafka-code-13.png"></p><p>Kafka 服务端主启动类中的 <code>main()</code> 方法，代码如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">main</span><span class="params">(args: Array[String])</span>: Unit </span>= {</span><br><span class="line"><span class="keyword">try</span> {</span><br><span class="line">    <span class="comment">// 获取服务端相关的配置参数</span></span><br><span class="line">    val serverProps = getPropsFromArgs(args)</span><br><span class="line">    <span class="comment">// 配置服务端</span></span><br><span class="line">    val server = buildServer(serverProps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> {</span><br><span class="line">    <span class="keyword">if</span> (!OperatingSystem.IS_WINDOWS &amp;&amp; !Java.isIbmJdk)</span><br><span class="line">        <span class="keyword">new</span> LoggingSignalHandler().register()</span><br><span class="line">    } <span class="keyword">catch</span> {</span><br><span class="line">    <span class="keyword">case</span> e: ReflectiveOperationException =&gt;</span><br><span class="line">        warn(<span class="string">"Failed to register optional signal handler that logs a message when the process is terminated "</span> +</span><br><span class="line">        s<span class="string">"by a signal. Reason for registration failure is: $e"</span>, e)</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// attach shutdown handler to catch terminating signals as well as normal termination</span></span><br><span class="line">    Exit.addShutdownHook(<span class="string">"kafka-shutdown-hook"</span>, {</span><br><span class="line">    <span class="keyword">try</span> server.shutdown()</span><br><span class="line">    <span class="keyword">catch</span> {</span><br><span class="line">        <span class="keyword">case</span> _: Throwable =&gt;</span><br><span class="line">        fatal(<span class="string">"Halting Kafka."</span>)</span><br><span class="line">        <span class="comment">// Calling exit() can lead to deadlock as exit() can be called multiple times. Force exit.</span></span><br><span class="line">        Exit.halt(<span class="number">1</span>)</span><br><span class="line">    }</span><br><span class="line">    })</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 启动服务端</span></span><br><span class="line">    <span class="keyword">try</span> server.startup()</span><br><span class="line">    <span class="keyword">catch</span> {</span><br><span class="line">    <span class="keyword">case</span> _: Throwable =&gt;</span><br><span class="line">        <span class="comment">// KafkaServer.startup() calls shutdown() in case of exceptions, so we invoke `exit` to set the status code</span></span><br><span class="line">        fatal(<span class="string">"Exiting Kafka."</span>)</span><br><span class="line">        Exit.exit(<span class="number">1</span>)</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    server.awaitShutdown()</span><br><span class="line">}</span><br><span class="line"><span class="keyword">catch</span> {</span><br><span class="line">    <span class="keyword">case</span> e: Throwable =&gt;</span><br><span class="line">    fatal(<span class="string">"Exiting Kafka due to fatal exception"</span>, e)</span><br><span class="line">    Exit.exit(<span class="number">1</span>)</span><br><span class="line">}</span><br><span class="line">Exit.exit(<span class="number">0</span>)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><div id="readmore-expansion" class="pjax"></div><link rel="stylesheet" type="text/css" href="https://qiniu.techgrow.cn/readmore/dist/hexo.css"><script data-pjax="" src="https://qiniu.techgrow.cn/readmore/dist/readmore.js" type="text/javascript"></script><script data-pjax="">var isMobile=navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i),allowMobile=!1;if(!isMobile||isMobile&&allowMobile)try{var plugin=new ReadmorePlugin;plugin.init({type:"hexo",id:"readmore-container",name:"全栈技术驿站",blogId:"96641-5333172926158-056",qrcode:"https://www.techgrow.cn/img/wx_mp_qr.png",keyword:"Tech",random:"1",height:"auto",expires:"365",lockToc:"yes",interval:"30",baseUrl:"",execute:"yes",tocSelector:""})}catch(e){console.warn("readmore plugin occurred error: "+e.name+" | "+e.message)}</script></div><footer class="post-footer"><div class="reward-container"><div>支持一根棒棒糖！</div> <button> 赞赏</button><div class="post-reward"><div> <img src="/img/pay_wx.png" alt="Clay 微信"> <span>微信</span></div><div> <img src="/img/pay_zfb.png" alt="Clay 支付宝"> <span>支付宝</span></div></div></div><div class="post-copyright"><ul><li class="post-copyright-author"> <strong>本文作者：</strong> Clay</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.techgrow.cn/posts/50c7d080.html" title="Kafka 入门教程之七">https://www.techgrow.cn/posts/50c7d080.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="contactme"><div class="social-list"><div class="social-item"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">欢迎添加博主微信，请备注 "博客"，届时会邀请您加入百人微信群</span><br> <img src="/img/wx_account_qr.png"></div></div></div><div class="post-tags"><a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/" rel="tag"><i class="fa fa-tag"></i> 分布式</a><a href="/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/" rel="tag"><i class="fa fa-tag"></i> 消息队列</a><a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag"><i class="fa fa-tag"></i> 大数据</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/b9ff615e.html" rel="prev" title="Vagrant 快速创建 VirtualBox 虚拟机"><i class="fa fa-angle-left"></i> Vagrant 快速创建 VirtualBox 虚拟机</a></div><div class="post-nav-item"> <a href="/posts/17b5887e.html" rel="next" title="Dubbo 3 入门教程之一">Dubbo 3 入门教程之一<i class="fa fa-angle-right"></i></a></div></div></footer></article></div><div class="comments" id="waline"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> Copyright © 2018 – <span itemprop="copyrightYear">2025</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Clay</span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-chart-line"></i></span> <span>站点总字数：</span> <span title="站点总字数">2.1m</span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span>站点阅读时长 ≈</span> <span title="站点阅读时长">31:13</span></span></div><div id="site-runtime"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span><span id="runtime"></span></div><script language="javascript">function isPC(){for(var e=navigator.userAgent,t=["Android","iPhone","SymbianOS","Windows Phone","iPad","iPod"],n=0;n<t.length;n++)if(0<e.indexOf(t[n]))return!1;return!0}function siteTime(e,t){window.setTimeout("siteTime(openOnPC, start)",1e3);var n=36e5,o=24*n;t=new Date("2018-12-27 08:00:00");var i=new Date,r=(i.getFullYear(),i.getMonth(),i.getDate(),i.getHours(),i.getMinutes(),i.getSeconds(),i-t),a=Math.floor(r/31536e6),s=Math.floor(r/o-365*a),d=Math.floor((r-(365*a+s)*o)/n),l=Math.floor((r-(365*a+s)*o-d*n)/6e4),u=Math.floor((r-(365*a+s)*o-d*n-6e4*l)/1e3);document.getElementById("runtime").innerHTML="Powered by Hexo & Docker | "+a+" 年 "+s+" 日 "+d+" 小时 "+l+" 分钟 "+u+" 秒 "}var showOnMobile=!1,openOnPC=isPC(),start=new Date;siteTime(openOnPC,start),openOnPC||showOnMobile||(document.getElementById("site-runtime").style.display="none")</script><div class="beian"> <span><img src="/img/gonganbeian.png" alt=""></span> <span><a href="https://beian.miit.gov.cn/" rel="external nofollow" target="_blank">粤ICP备 19024664号-1</a></span> <span>|&nbsp;</span> <span><a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=44011302004035" rel="external nofollow" target="_blank">粤公网安备 44011302004035号</a></span></div><div class="busuanzi-count"><span class="post-meta-item" id="busuanzi_container_site_uv"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span></span></span><span class="post-meta-item" id="busuanzi_container_site_pv"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer><div class="toggle sidebar-toggle" role="button"><span class="toggle-line"></span><span class="toggle-line"></span><span class="toggle-line"></span></div><div class="sidebar-dimmer"></div> <a href="https://github.com/rqh656418510" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="external nofollow" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><noscript><div class="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><script size="200" alpha="0.5" zindex="-1" src="/lib/ribbon.js/dist/ribbon.min.js"></script><script src="/lib/animejs/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script><script src="/lib/@next-theme/pjax/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script><script src="/lib/medium-zoom/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script><script src="/lib/lozad/dist/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script><script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script><script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"/lib/pdfobject/pdfobject.min.js","integrity":"sha256-JJZNsid68vnh3/zyj0lY9BN5ynxVX/12XgOa1TlaYN0="},"url":"/lib/pdf/web/viewer.html"}</script><script src="/js/third-party/tags/pdf.js"></script><script src="/js/third-party/pace.js"></script><script data-pjax="" async="" src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://www.techgrow.cn/lib/darkmode/darkmode@1.5.7.min.js"></script><script>
var options = {
  bottom: '64px',
  right: '30px',
  left: 'unset',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#282828',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script><script src="https://www.techgrow.cn/lib/qiniu/qiniu@3.3.1.min.js"></script><script>
    var qiniu_domain = "https://oss.techgrow.cn";
    var qiniu_token_url = "https://open.techgrow.cn/api/oss/qiniu/token/upload";
    var qiniu_debug = "true" === "false";

    // date format
    Date.prototype.format = function (fmt) {
      var o = {
        "M+": this.getMonth() + 1,
        "d+": this.getDate(),
        "h+": this.getHours(),
        "m+": this.getMinutes(),
        "s+": this.getSeconds(),
        "q+": Math.floor((this.getMonth() + 3) / 3),
        "S": this.getMilliseconds()
      };
      if (/(y+)/.test(fmt)) {
        fmt = fmt.replace(RegExp.$1, (this.getFullYear() + "").substr(4 - RegExp.$1.length));
      }
      for (var k in o) {
        if (new RegExp("(" + k + ")").test(fmt)) {
          fmt = fmt.replace(
            RegExp.$1,
            (RegExp.$1.length == 1)
              ? (o[k])
              : (("00" + o[k]).substr(("" + o[k]).length))
          );
        }
      }
      return fmt;
    }

    // generate uuid
    function uuid() {
      var s = [];
      var hexDigits = "0123456789abcdef";
      for (var i = 0; i < 36; i++) {
        s[i] = hexDigits.substr(Math.floor(Math.random() * 0x10), 1);
      }
      s[14] = "4";
      s[19] = hexDigits.substr((s[19] & 0x3) | 0x8, 1);
      s[8] = s[13] = s[18] = s[23] = "-";
      var uuid = s.join("");
      return uuid;
    }

    // sync get request
    function syncGet(url) {
      var xhr = null;
      if (window.XMLHttpRequest) {
        xhr = new XMLHttpRequest();
      } else {
        xhr = new ActiveXObject("Microsoft.XMLHTTP");
      }
      xhr.open('GET', url, false);
      xhr.send();
      return xhr;
    }

    // get upload file path
    function getUploadFilePath() {
      var now = new Date();
      var name = uuid().replace(/-/g, "");
      var nowStr = now.format("/yyyy/MM/dd/");
      return "uploads" + nowStr + name;
    }

    // get qiniu upload token
    function getUploadToken() {
      try {
        var xhr = syncGet(qiniu_token_url);
        var responseStatus = xhr.status;
        var responseJson = JSON.parse(xhr.responseText);
        if (responseStatus === 200) {
          return responseJson.data;
        } else if (responseStatus === 403) {
          alert(responseJson.msg || "图片上传失败，无法获取UploadToken，非法请求来源！");
        } else if (responseStatus === 429) {
          alert(responseJson.msg || "图片上传失败，无法获取UploadToken，上传过于频繁！");
        } else if (responseStatus === 500) {
          alert(responseJson.msg || "图片上传失败，无法获取UploadToken，系统内部出错！");
        } else {
          alert("图片上传失败，无法获取UploadToken，未知Http响应状态码！");
        }
      } catch (err) {
        if (qiniu_debug) {
          console.error(err);
        }
        alert("图片上传失败，无法获取UploadToken，未知错误！");
      }
      return null;
    }

    // qiniu upload image
    async function qiniuUploadImage(file) {
      var image_path = null;
      await uploadImage(file).then(function onFulfilled(res) {
        image_path = res;
      }).catch(function onRejected(err) {
        if (qiniu_debug) {
          console.error(err);
        }
      });
      return image_path;
    }

    // upload image
    function uploadImage(file) {
      return new Promise((resolve, reject) => {
        var config = null;
        var putExtra = null;
        var token = getUploadToken();
        var key = getUploadFilePath();
        // upload init
        var observable = qiniu.upload(file, key, token, putExtra, config);
        // upload start
        observable.subscribe({
          next(res) {
            // upload progress
          },
          error(err) {
            // upload falied
            reject("falied to upload image for qiniu: " + err.name);
          },
          complete(res) {
            // upload successed
            resolve(qiniu_domain + "/" + key);
          }
        });
      });
    }
  </script><script class="next-config" data-name="waline" type="application/json">{"lang":"zh-CN","enable":true,"serverURL":"https://waline.techgrow.cn","cssUrl":"https://www.techgrow.cn/lib/@waline/client/client@2.5.1.min.css","commentCount":true,"pageview":false,"copyright":false,"allowUploadImage":true,"libUrl":"https://www.techgrow.cn/lib/@waline/client/client@2.5.1.min.js","locale":{"placeholder":"支持匿名评论啦，若希望及时收到博主的反馈，建议登录评论或者在上方的邮箱输入框留下邮箱地址哦 (๑•̀ㅂ•́)و✧"},"dark":"body.darkmode--activated","emoji":["https://www.techgrow.cn/lib/@waline/emojis/1.0.1/weibo"],"meta":["nick","mail","link"],"login":"enable","pageSize":10,"qiniuDebug":false,"qiniuDomain":"https://oss.techgrow.cn","qiniuTokenUrl":"https://open.techgrow.cn/api/oss/qiniu/token/upload","qiniuLibUrl":"https://www.techgrow.cn/lib/qiniu/qiniu@3.3.1.min.js","el":"#waline","comment":true,"path":"/posts/50c7d080.html"}</script><link rel="stylesheet" href="https://www.techgrow.cn/lib/@waline/client/client@2.5.1.min.css"><script>
document.addEventListener('page:loaded', () => {
  if (!CONFIG.waline.allowUploadImage) {
    CONFIG.waline.imageUploader = false;
  }
  else if (CONFIG.waline.qiniuDomain && CONFIG.waline.qiniuTokenUrl) {
    CONFIG.waline.imageUploader = qiniuUploadImage;
  } else {
   CONFIG.waline.imageUploader = true;
  }
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script><div class="moon-menu"><div class="moon-menu-items"><div id="moon-menu-item-back2bottom" class="moon-menu-item"><i class="fas fa-chevron-down"></i></div><div id="moon-menu-item-back2top" class="moon-menu-item"><i class="fas fa-chevron-up"></i></div></div><div class="moon-menu-button"><svg class="moon-menu-bg"><circle class="moon-menu-cricle" cx="50%" cy="50%" r="44%"></circle><circle class="moon-menu-border" cx="50%" cy="50%" r="48%"></circle></svg><div class="moon-menu-content"><div class="moon-menu-icon"><i class="fas fa-ellipsis-v"></i></div><div class="moon-menu-text"></div></div></div></div><script src="/js/injector.js"></script><div class="comments" id="waline-comments" style="display:none"></div><script>function isMobile(){var i=navigator.userAgent.toLowerCase(),e="ipad"==i.match(/ipad/i),a="iphone os"==i.match(/iphone os/i),o="midp"==i.match(/midp/i),n="rv:1.2.3.4"==i.match(/rv:1.2.3.4/i),r="ucweb"==i.match(/ucweb/i),c="android"==i.match(/android/i),l="windows ce"==i.match(/windows ce/i),d="windows mobile"==i.match(/windows mobile/i);return!!(e||a||o||n||r||c||l||d)}var openOnMobile=isMobile(),showOnMobile=!1,aplayerEnable=!0;jQuery(document).ready(function(){aplayerEnable&&(openOnMobile&&!showOnMobile||jQuery("#aplayer").css("display","block"))}),jQuery(window).on("load",function(){aplayerEnable&&jQuery(".aplayer\\-icon.aplayer\\-icon\\-lrc").trigger("click")})</script></body></html>